{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Combined Data Pipeline - Cameron H\n",
        "This notebook combines data cleaning, feature engineering, and modeling from codebook.ipynb and data_cleaning.ipynb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "# IMPORTS\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import svm\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.metrics import precision_score, recall_score, roc_curve as sklearn_roc_curve\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CUSTOM METRIC FUNCTIONS\n",
        "\n",
        "def roc_auc(actual, preds):\n",
        "    \"\"\"Manual ROC-AUC implementation.\"\"\"\n",
        "    actual = np.array(actual) == 1\n",
        "    tpr = []\n",
        "    fpr = []\n",
        "    for thresh in [x / 100.0 for x in range(0, 101)]:\n",
        "        preds_t = np.array(preds) >= thresh\n",
        "        tp = sum(preds_t & actual)\n",
        "        fp = sum(preds_t & ~actual)\n",
        "        tn = sum(~preds_t & ~actual)\n",
        "        fn = sum(~preds_t & actual)\n",
        "        tpr.append(tp / (tp + fn))\n",
        "        fpr.append(tn / (tn + fp))\n",
        "\n",
        "    auc = 0\n",
        "    for i in range(0, len(tpr) - 1):\n",
        "        auc += ((tpr[i] + tpr[i + 1]) / 2) * (fpr[i + 1] - fpr[i])\n",
        "    return auc\n",
        "\n",
        "\n",
        "def accuracy(actual, preds, thresh):\n",
        "    \"\"\"Calculate accuracy at given threshold.\"\"\"\n",
        "    preds = np.array(preds) >= thresh\n",
        "    actual = np.array(actual) == 1\n",
        "    actual = (actual == 1)\n",
        "    acc = np.count_nonzero(preds == actual)/len(actual)\n",
        "    return acc\n",
        "\n",
        "def precision(actual, preds, thresh):\n",
        "    \"\"\"Calculate precision at given threshold.\"\"\"\n",
        "    preds = (np.array(preds) >= thresh)\n",
        "    actual = (np.array(actual) == 1)\n",
        "    tp = np.count_nonzero(preds & actual)\n",
        "    fp = np.count_nonzero(preds & ~actual)\n",
        "    prec = tp/(tp+fp) if (tp+fp) > 0 else 0\n",
        "    return prec\n",
        "\n",
        "def recall(actual, preds, thresh):\n",
        "    \"\"\"Calculate recall (sensitivity) at given threshold.\"\"\"\n",
        "    preds = (np.array(preds) >= thresh)\n",
        "    actual = (np.array(actual) == 1)\n",
        "    tp = np.count_nonzero(preds & actual)\n",
        "    fn = np.count_nonzero(~preds & actual)\n",
        "    rec = tp/(tp+fn) if (tp+fn) > 0 else 0\n",
        "    return rec\n",
        "\n",
        "def f1_score(actual, preds, thresh):\n",
        "    \"\"\"Calculate F1 score at given threshold.\"\"\"\n",
        "    preds = (np.array(preds) >= thresh)\n",
        "    actual = (np.array(actual) == 1)\n",
        "    tp = np.count_nonzero(preds & actual)\n",
        "    fp = np.count_nonzero(preds & ~actual)\n",
        "    fn = np.count_nonzero(~preds & actual)\n",
        "    rec = tp/(tp+fn) if (tp+fn) > 0 else 0\n",
        "    prec = tp/(tp+fp) if (tp+fp) > 0 else 0\n",
        "    f1 = (2 * rec * prec)/(rec + prec) if (rec + prec) > 0 else 0\n",
        "    return f1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Application train shape: (307511, 122)\n",
            "Bureau shape: (1716428, 17)\n",
            "Previous application shape: (1670214, 37)\n",
            "\n",
            "Note: Bureau and Previous Application have multiple rows per customer.\n",
            "They will be aggregated (summed/counted) before merging into the main dataset.\n"
          ]
        }
      ],
      "source": [
        "# 1. LOAD DATA\n",
        "# \n",
        "# DATA STRUCTURE EXPLANATION:\n",
        "# - application_train_df: MAIN dataset (1 row per customer/application) - this is our base\n",
        "# - bureau_df: Credit bureau history (MULTIPLE rows per customer - will be aggregated)\n",
        "# - prev_app_df: Previous loan applications (MULTIPLE rows per customer - will be aggregated)\n",
        "#\n",
        "# PROCESS: We'll merge bureau and previous_application data INTO application_train_df\n",
        "#          by aggregating them first (sum/count per customer), then adding as new columns\n",
        "# Notebook CWD: c:\\Users\\nerdc\\OneDrive\\Documents\\R Projects\\DATA403-Project-2\\cameron_work\n",
        "application_train_df = pd.read_csv('application_train.csv')\n",
        "bureau_df = pd.read_csv('bureau.csv')\n",
        "prev_app_df = pd.read_csv('previous_application.csv')\n",
        "\n",
        "print(f\"Application train shape: {application_train_df.shape}\")\n",
        "print(f\"Bureau shape: {bureau_df.shape}\")\n",
        "print(f\"Previous application shape: {prev_app_df.shape}\")\n",
        "print(\"\\nNote: Bureau and Previous Application have multiple rows per customer.\")\n",
        "print(\"They will be aggregated (summed/counted) before merging into the main dataset.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Subset application train shape: (30000, 122)\n",
            "Number of unique customers in sample: 30000\n"
          ]
        }
      ],
      "source": [
        "# 2. SUBSET DATA FOR PERFORMANCE\n",
        "# Take a random subset to ensure the notebook can run efficiently\n",
        "# NOTE: Sampling BEFORE aggregation is fine because:\n",
        "# - We'll only merge bureau/prev_app records for customers in our sample (using how='left')\n",
        "# - Aggregation groups by SK_ID_CURR, so each customer gets correct aggregated values\n",
        "# - Customers without bureau/prev_app records will get 0 (filled later)\n",
        "SUBSET_SIZE = 30000\n",
        "application_train_df = application_train_df.sample(n=min(SUBSET_SIZE, len(application_train_df)), random_state=42)\n",
        "\n",
        "# Get list of customer IDs in our sample (for efficiency when merging)\n",
        "sample_customer_ids = set(application_train_df['SK_ID_CURR'].values)\n",
        "\n",
        "print(f\"Subset application train shape: {application_train_df.shape}\")\n",
        "print(f\"Number of unique customers in sample: {len(sample_customer_ids)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Remaining NaNs: 0\n",
            "Shape after cleaning: (30000, 244)\n"
          ]
        }
      ],
      "source": [
        "# 3. DATA CLEANING\n",
        "\n",
        "# DAYS_EMPLOYED = 365243 means \"no employment record\"\n",
        "application_train_df[\"DAYS_EMPLOYED\"] = application_train_df[\"DAYS_EMPLOYED\"].replace(365243, np.nan)\n",
        "\n",
        "# Create missingness indicators (best-practice for this dataset)\n",
        "missing_cols = {}\n",
        "for col in application_train_df.columns:\n",
        "    missing_cols[col + \"_MISSING\"] = application_train_df[col].isna().astype(int)\n",
        "\n",
        "# Add missing indicators efficiently\n",
        "missing_df = pd.DataFrame(missing_cols, index=application_train_df.index)\n",
        "application_train_df = pd.concat([application_train_df, missing_df], axis=1)\n",
        "\n",
        "# Identify categorical columns\n",
        "categorical_cols = application_train_df.select_dtypes(include=[\"object\"]).columns\n",
        "\n",
        "# Coerce numeric-like columns\n",
        "numeric_like_cols = application_train_df.columns.difference(categorical_cols)\n",
        "application_train_df[numeric_like_cols] = application_train_df[numeric_like_cols].apply(\n",
        "    lambda col: pd.to_numeric(col, errors=\"coerce\")\n",
        ")\n",
        "\n",
        "# Set up numeric & categorical column lists\n",
        "numeric_cols = application_train_df.select_dtypes(include=[np.number]).columns\n",
        "numeric_cols = numeric_cols.drop(\"TARGET\")  # do NOT impute target\n",
        "\n",
        "# Impute numerics (median) & categoricals (mode)\n",
        "num_imputer = SimpleImputer(strategy=\"median\")\n",
        "application_train_df[numeric_cols] = num_imputer.fit_transform(application_train_df[numeric_cols])\n",
        "\n",
        "cat_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
        "application_train_df[categorical_cols] = cat_imputer.fit_transform(application_train_df[categorical_cols])\n",
        "\n",
        "# Confirm no NaNs\n",
        "print(\"Remaining NaNs:\", application_train_df.isna().sum().sum())\n",
        "print(f\"Shape after cleaning: {application_train_df.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape after bureau merge: (30000, 248)\n"
          ]
        }
      ],
      "source": [
        "# 4. FEATURE ENGINEERING - BUREAU DATA\n",
        "# \n",
        "# Bureau data has multiple credit records per customer. We aggregate them into summary features:\n",
        "# - Total overdue amount per customer\n",
        "# - Total debt per customer  \n",
        "# - Total times credit was prolonged per customer\n",
        "# - Total days overdue per customer\n",
        "# Then we add these as NEW COLUMNS to the main dataset (one value per customer)\n",
        "\n",
        "# Filter bureau data to only include records for customers in our sample (for efficiency)\n",
        "bureau_filtered = bureau_df[bureau_df['SK_ID_CURR'].isin(sample_customer_ids)].copy()\n",
        "\n",
        "# Merge with bureau data (creates temporary merged df with multiple rows per customer)\n",
        "application_bureau_df = pd.merge(application_train_df, bureau_filtered, on=\"SK_ID_CURR\", how=\"left\")\n",
        "\n",
        "# Aggregate bureau features\n",
        "total_overdue = application_bureau_df.groupby(\"SK_ID_CURR\")[\"AMT_CREDIT_SUM_OVERDUE\"].sum()\n",
        "total_debt = application_bureau_df.groupby(\"SK_ID_CURR\")[\"AMT_CREDIT_SUM_DEBT\"].sum()\n",
        "times_prolonged = application_bureau_df.groupby(\"SK_ID_CURR\")[\"CNT_CREDIT_PROLONG\"].sum()\n",
        "days_overdue = application_bureau_df.groupby(\"SK_ID_CURR\")[\"CREDIT_DAY_OVERDUE\"].sum()\n",
        "\n",
        "# Merge aggregated features back\n",
        "application_train_merged_df = application_train_df.merge(total_overdue, on='SK_ID_CURR', how='left')\n",
        "application_train_merged_df = application_train_merged_df.merge(total_debt, on='SK_ID_CURR', how='left')\n",
        "application_train_merged_df = application_train_merged_df.merge(times_prolonged, on='SK_ID_CURR', how='left')\n",
        "application_train_merged_df = application_train_merged_df.merge(days_overdue, on=\"SK_ID_CURR\", how='left')\n",
        "\n",
        "# Fill NaN values with 0 for new features\n",
        "application_train_merged_df['AMT_CREDIT_SUM_OVERDUE'] = application_train_merged_df['AMT_CREDIT_SUM_OVERDUE'].fillna(0)\n",
        "application_train_merged_df['AMT_CREDIT_SUM_DEBT'] = application_train_merged_df['AMT_CREDIT_SUM_DEBT'].fillna(0)\n",
        "application_train_merged_df['CNT_CREDIT_PROLONG'] = application_train_merged_df['CNT_CREDIT_PROLONG'].fillna(0)\n",
        "application_train_merged_df['CREDIT_DAY_OVERDUE'] = application_train_merged_df['CREDIT_DAY_OVERDUE'].fillna(0)\n",
        "\n",
        "print(f\"Shape after bureau merge: {application_train_merged_df.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape after previous application merge: (30000, 250)\n"
          ]
        }
      ],
      "source": [
        "# 5. FEATURE ENGINEERING - PREVIOUS APPLICATION DATA\n",
        "#\n",
        "# Previous application data has multiple applications per customer. We create summary features:\n",
        "# - PREV_APPS: Count of total previous applications per customer\n",
        "# - NUM_APPROVED: Count of approved previous applications per customer\n",
        "# Then we add these as NEW COLUMNS to the main dataset (one value per customer)\n",
        "\n",
        "# Filter previous application data to only include records for customers in our sample (for efficiency)\n",
        "prev_app_filtered = prev_app_df[prev_app_df['SK_ID_CURR'].isin(sample_customer_ids)].copy()\n",
        "\n",
        "# Count previous applications per customer\n",
        "prev_app_ct = prev_app_filtered[[\"SK_ID_CURR\"]].copy()\n",
        "prev_app_ct[\"PREV_APPS\"] = 1\n",
        "prev_app_ct = prev_app_ct.groupby(\"SK_ID_CURR\")[\"PREV_APPS\"].count().reset_index()\n",
        "\n",
        "# Count approved previous applications\n",
        "if \"NAME_CONTRACT_STATUS\" in prev_app_filtered.columns:\n",
        "    prev_app_approved = prev_app_filtered[[\"SK_ID_CURR\", \"NAME_CONTRACT_STATUS\"]].copy()\n",
        "    prev_app_approved[\"NUM_APPROVED\"] = np.where(prev_app_approved[\"NAME_CONTRACT_STATUS\"] == \"Approved\", 1, 0)\n",
        "    prev_app_approved = prev_app_approved.groupby(\"SK_ID_CURR\")[\"NUM_APPROVED\"].sum().reset_index()\n",
        "    \n",
        "    # Merge counts\n",
        "    prev_app_ct = prev_app_ct.merge(prev_app_approved, on=\"SK_ID_CURR\", how=\"left\")\n",
        "    prev_app_ct[\"NUM_APPROVED\"] = prev_app_ct[\"NUM_APPROVED\"].fillna(0)\n",
        "else:\n",
        "    # If NAME_CONTRACT_STATUS doesn't exist, just use PREV_APPS\n",
        "    prev_app_ct[\"NUM_APPROVED\"] = 0\n",
        "\n",
        "# Merge with main dataframe\n",
        "application_train_merged_df = application_train_merged_df.merge(prev_app_ct, on=\"SK_ID_CURR\", how=\"left\")\n",
        "application_train_merged_df['PREV_APPS'] = application_train_merged_df['PREV_APPS'].fillna(0)\n",
        "application_train_merged_df['NUM_APPROVED'] = application_train_merged_df['NUM_APPROVED'].fillna(0)\n",
        "\n",
        "print(f\"Shape after previous application merge: {application_train_merged_df.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nscaler = StandardScaler()\\ncols_to_standardize = [\\'AMT_CREDIT_SUM_OVERDUE\\', \\'AMT_CREDIT_SUM_DEBT\\', \\'CNT_CREDIT_PROLONG\\', \\n                       \\'CREDIT_DAY_OVERDUE\\', \\'PREV_APPS\\', \\'NUM_APPROVED\\']\\n\\n# Only standardize columns that exist\\ncols_to_standardize = [col for col in cols_to_standardize if col in application_train_merged_df.columns]\\n\\nif cols_to_standardize:\\n    application_train_merged_df[cols_to_standardize] = scaler.fit_transform(application_train_merged_df[cols_to_standardize])\\n    print(f\"Standardized columns: {cols_to_standardize}\")\\nelse:\\n    print(\"No columns to standardize\")\\n'"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# We could get rid of this code cell because we standardize later on anyway\n",
        "# 6. STANDARDIZE NEW FEATURES\n",
        "\"\"\"\n",
        "scaler = StandardScaler()\n",
        "cols_to_standardize = ['AMT_CREDIT_SUM_OVERDUE', 'AMT_CREDIT_SUM_DEBT', 'CNT_CREDIT_PROLONG', \n",
        "                       'CREDIT_DAY_OVERDUE', 'PREV_APPS', 'NUM_APPROVED']\n",
        "\n",
        "# Only standardize columns that exist\n",
        "cols_to_standardize = [col for col in cols_to_standardize if col in application_train_merged_df.columns]\n",
        "\n",
        "if cols_to_standardize:\n",
        "    application_train_merged_df[cols_to_standardize] = scaler.fit_transform(application_train_merged_df[cols_to_standardize])\n",
        "    print(f\"Standardized columns: {cols_to_standardize}\")\n",
        "else:\n",
        "    print(\"No columns to standardize\")\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape after one-hot encoding: (30000, 357)\n"
          ]
        }
      ],
      "source": [
        "# 7. ONE-HOT ENCODE CATEGORICAL FIELDS\n",
        "\n",
        "df_encoded = pd.get_dummies(application_train_merged_df, drop_first=True)\n",
        "print(f\"Shape after one-hot encoding: {df_encoded.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dropped 33 low correlation columns\n",
            "Final shape: (30000, 324)\n"
          ]
        }
      ],
      "source": [
        "# 8. CORRELATION-BASED FILTERING\n",
        "\n",
        "# Only compute correlations on numeric columns\n",
        "numeric_df = df_encoded.select_dtypes(include=[np.number])\n",
        "correlations = numeric_df.corr()[\"TARGET\"]\n",
        "\n",
        "# Filter out low correlation columns\n",
        "low_corr_cols = correlations[abs(correlations) < 0.01].index.tolist()\n",
        "\n",
        "# DO NOT drop TARGET even if correlation calculation returns it\n",
        "low_corr_cols = [col for col in low_corr_cols if col not in [\"TARGET\", \"SK_ID_CURR\"]]\n",
        "\n",
        "df_filtered = df_encoded.drop(columns=low_corr_cols, errors='ignore')\n",
        "\n",
        "print(f\"Dropped {len(low_corr_cols)} low correlation columns\")\n",
        "print(f\"Final shape: {df_filtered.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final X shape: (30000, 323)\n",
            "Final y shape: (30000,)\n",
            "Target distribution: {0: 27543, 1: 2457}\n",
            "Target proportion - Class 0: 0.918, Class 1: 0.082\n",
            "\n",
            "======================================================================\n",
            "SCALING ALL FEATURES FOR MODELING EXCEPT SK_ID_CURR\n",
            "======================================================================\n",
            "Scaled all 323 features (mean=0, std=1)\n",
            "This should help:\n",
            "  - Logistic Regression: Better convergence, no more warnings\n",
            "  - SVM: Improved performance (SVM is sensitive to feature scale)\n",
            "  - LDA: Unaffected (already handles scaled features well)\n",
            "\n",
            "X ready for modeling: (30000, 323)\n"
          ]
        }
      ],
      "source": [
        "# 9. PREPARE FOR MODELING\n",
        "\n",
        "# Remove any remaining NaNs\n",
        "df_filtered = df_filtered.dropna()\n",
        "\n",
        "X = df_filtered.drop(\"TARGET\", axis=1)\n",
        "y = df_filtered[\"TARGET\"]\n",
        "\n",
        "print(f\"Final X shape: {X.shape}\")\n",
        "print(f\"Final y shape: {y.shape}\")\n",
        "print(f\"Target distribution: {y.value_counts().to_dict()}\")\n",
        "print(f\"Target proportion - Class 0: {y.value_counts()[0]/len(y):.3f}, Class 1: {y.value_counts()[1]/len(y):.3f}\")\n",
        "\n",
        "# 9B. SCALE ALL FEATURES FOR MODELING\n",
        "# Recommendation: Scale all features to help Logistic Regression converge and improve SVM performance\n",
        "# This addresses the ConvergenceWarning and should improve model performance\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"SCALING ALL FEATURES FOR MODELING EXCEPT SK_ID_CURR\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Create scaler for all features\n",
        "feature_scaler = StandardScaler()\n",
        "\n",
        "# Store column names before scaling (scaling converts to numpy array)\n",
        "feature_names = [col for col in X.columns if col != \"SK_ID_CURR\"]\n",
        "\n",
        "X_scaled = X.copy()\n",
        "X_scaled[feature_names] = feature_scaler.fit_transform(X[feature_names])\n",
        "\n",
        "# Scale all features\n",
        "#X_scaled = feature_scaler.fit_transform(X)\n",
        "#X_scaled = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)\n",
        "\n",
        "print(f\"Scaled all {X_scaled.shape[1]} features (mean=0, std=1)\")\n",
        "print(\"This should help:\")\n",
        "print(\"  - Logistic Regression: Better convergence, no more warnings\")\n",
        "print(\"  - SVM: Improved performance (SVM is sensitive to feature scale)\")\n",
        "print(\"  - LDA: Unaffected (already handles scaled features well)\")\n",
        "\n",
        "# Use scaled features for modeling\n",
        "X = X_scaled.copy()\n",
        "\n",
        "print(f\"\\nX ready for modeling: {X.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing split functions...\n",
            "\n",
            "Random Split:\n",
            "  Train: 24000 samples, Class 0: 22007, Class 1: 1993\n",
            "  Test:  6000 samples, Class 0: 5536, Class 1: 464\n",
            "\n",
            "Stratified Split:\n",
            "  Train: 24001 samples, Class 0: 22035, Class 1: 1966\n",
            "  Test:  5999 samples, Class 0: 5508, Class 1: 491\n",
            "\n",
            "Non-Random Split:\n",
            "  Train: 24000 samples, Class 0: 22031, Class 1: 1969\n",
            "  Test:  6000 samples, Class 0: 5512, Class 1: 488\n"
          ]
        }
      ],
      "source": [
        "# 10. DATA SPLITTING FUNCTIONS (FROM SCRATCH)\n",
        "# Assignment requirement: Random split, Stratified split, Non-random split\n",
        "\n",
        "def random_split(X, y, test_size=0.2, random_state=42):\n",
        "    \"\"\"\n",
        "    Random train/test split implementation from scratch.\n",
        "    \n",
        "    Parameters:\n",
        "    - X: Features dataframe\n",
        "    - y: Target series\n",
        "    - test_size: Proportion of data for test set (default 0.2)\n",
        "    - random_state: Random seed for reproducibility\n",
        "    \n",
        "    Returns:\n",
        "    - X_train, X_test, y_train, y_test\n",
        "    \"\"\"\n",
        "    np.random.seed(random_state)\n",
        "    n_samples = len(X)\n",
        "    n_test = int(n_samples * test_size)\n",
        "    \n",
        "    # Create random indices\n",
        "    indices = np.arange(n_samples)\n",
        "    np.random.shuffle(indices)\n",
        "    \n",
        "    # Split indices\n",
        "    test_indices = indices[:n_test]\n",
        "    train_indices = indices[n_test:]\n",
        "    \n",
        "    # Split data\n",
        "    X_train = X.iloc[train_indices].reset_index(drop=True)\n",
        "    X_test = X.iloc[test_indices].reset_index(drop=True)\n",
        "    y_train = y.iloc[train_indices].reset_index(drop=True)\n",
        "    y_test = y.iloc[test_indices].reset_index(drop=True)\n",
        "    \n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "\n",
        "def stratified_split(X, y, test_size=0.2, random_state=42):\n",
        "    \"\"\"\n",
        "    Stratified train/test split implementation from scratch.\n",
        "    Maintains class proportions in both train and test sets.\n",
        "    \n",
        "    Parameters:\n",
        "    - X: Features dataframe\n",
        "    - y: Target series\n",
        "    - test_size: Proportion of data for test set (default 0.2)\n",
        "    - random_state: Random seed for reproducibility\n",
        "    \n",
        "    Returns:\n",
        "    - X_train, X_test, y_train, y_test\n",
        "    \"\"\"\n",
        "    np.random.seed(random_state)\n",
        "    \n",
        "    # Get indices for each class\n",
        "    class_0_indices = np.where(y == 0)[0]\n",
        "    class_1_indices = np.where(y == 1)[0]\n",
        "    \n",
        "    # Shuffle each class's indices\n",
        "    np.random.shuffle(class_0_indices)\n",
        "    np.random.shuffle(class_1_indices)\n",
        "    \n",
        "    # Calculate test size for each class\n",
        "    n_test_0 = int(len(class_0_indices) * test_size)\n",
        "    n_test_1 = int(len(class_1_indices) * test_size)\n",
        "    \n",
        "    # Split indices for each class\n",
        "    test_indices_0 = class_0_indices[:n_test_0]\n",
        "    train_indices_0 = class_0_indices[n_test_0:]\n",
        "    \n",
        "    test_indices_1 = class_1_indices[:n_test_1]\n",
        "    train_indices_1 = class_1_indices[n_test_1:]\n",
        "    \n",
        "    # Combine indices\n",
        "    test_indices = np.concatenate([test_indices_0, test_indices_1])\n",
        "    train_indices = np.concatenate([train_indices_0, train_indices_1])\n",
        "    \n",
        "    # Shuffle combined indices\n",
        "    np.random.shuffle(test_indices)\n",
        "    np.random.shuffle(train_indices)\n",
        "    \n",
        "    # Split data\n",
        "    X_train = X.iloc[train_indices].reset_index(drop=True)\n",
        "    X_test = X.iloc[test_indices].reset_index(drop=True)\n",
        "    y_train = y.iloc[train_indices].reset_index(drop=True)\n",
        "    y_test = y.iloc[test_indices].reset_index(drop=True)\n",
        "    \n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "\n",
        "def non_random_split(X, y, test_size=0.2):\n",
        "    \"\"\"\n",
        "    Non-random train/test split implementation from scratch.\n",
        "    Uses first (1-test_size) for training, last test_size for testing.\n",
        "    This simulates temporal split or preserves data order.\n",
        "    \n",
        "    Parameters:\n",
        "    - X: Features dataframe\n",
        "    - y: Target series\n",
        "    - test_size: Proportion of data for test set (default 0.2)\n",
        "    \n",
        "    Returns:\n",
        "    - X_train, X_test, y_train, y_test\n",
        "    \"\"\"\n",
        "    n_samples = len(X)\n",
        "    n_test = int(n_samples * test_size)\n",
        "    n_train = n_samples - n_test\n",
        "    \n",
        "    # Split by order (first n_train for train, last n_test for test)\n",
        "    train_indices = np.arange(n_train)\n",
        "    test_indices = np.arange(n_train, n_samples)\n",
        "    \n",
        "    # Split data\n",
        "    X_train = X.iloc[train_indices].reset_index(drop=True)\n",
        "    X_test = X.iloc[test_indices].reset_index(drop=True)\n",
        "    y_train = y.iloc[train_indices].reset_index(drop=True)\n",
        "    y_test = y.iloc[test_indices].reset_index(drop=True)\n",
        "    \n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "\n",
        "# Test the split functions\n",
        "print(\"Testing split functions...\")\n",
        "X_train_rand, X_test_rand, y_train_rand, y_test_rand = random_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train_strat, X_test_strat, y_train_strat, y_test_strat = stratified_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train_nonrand, X_test_nonrand, y_train_nonrand, y_test_nonrand = non_random_split(X, y, test_size=0.2)\n",
        "\n",
        "print(f\"\\nRandom Split:\")\n",
        "print(f\"  Train: {len(X_train_rand)} samples, Class 0: {(y_train_rand==0).sum()}, Class 1: {(y_train_rand==1).sum()}\")\n",
        "print(f\"  Test:  {len(X_test_rand)} samples, Class 0: {(y_test_rand==0).sum()}, Class 1: {(y_test_rand==1).sum()}\")\n",
        "\n",
        "print(f\"\\nStratified Split:\")\n",
        "print(f\"  Train: {len(X_train_strat)} samples, Class 0: {(y_train_strat==0).sum()}, Class 1: {(y_train_strat==1).sum()}\")\n",
        "print(f\"  Test:  {len(X_test_strat)} samples, Class 0: {(y_test_strat==0).sum()}, Class 1: {(y_test_strat==1).sum()}\")\n",
        "\n",
        "print(f\"\\nNon-Random Split:\")\n",
        "print(f\"  Train: {len(X_train_nonrand)} samples, Class 0: {(y_train_nonrand==0).sum()}, Class 1: {(y_train_nonrand==1).sum()}\")\n",
        "print(f\"  Test:  {len(X_test_nonrand)} samples, Class 0: {(y_test_nonrand==0).sum()}, Class 1: {(y_test_nonrand==1).sum()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "COMPREHENSIVE SPLIT STRATEGY COMPARISON\n",
            "======================================================================\n",
            "\n",
            "This will train all 3 models on all 3 split strategies...\n",
            "Total: 3 models Ã— 3 splits = 9 model trainings\n",
            "\n",
            "\n",
            "======================================================================\n",
            "SPLIT STRATEGY: RANDOM\n",
            "======================================================================\n",
            "Train: 24000 samples | Test: 6000 samples\n",
            "Train class dist: 0=22007, 1=1993\n",
            "Test class dist:  0=5536, 1=464\n",
            "\n",
            "  Training Logistic Regression...\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 55\u001b[39m\n\u001b[32m     52\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m  Training \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_split\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_split\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# Get predictions\u001b[39;00m\n\u001b[32m     58\u001b[39m y_pred_proba_test = model.predict_proba(X_test_split)[:, \u001b[32m1\u001b[39m]\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nerdc\\.venvs\\data403\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nerdc\\.venvs\\data403\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1384\u001b[39m, in \u001b[36mLogisticRegression.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m   1381\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1382\u001b[39m     n_threads = \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1384\u001b[39m fold_coefs_ = \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1385\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1386\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1387\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1388\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpos_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclass_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1389\u001b[39m \u001b[43m        \u001b[49m\u001b[43mCs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mC_\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1390\u001b[39m \u001b[43m        \u001b[49m\u001b[43ml1_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43ml1_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1391\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfit_intercept\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit_intercept\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1392\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1393\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1394\u001b[39m \u001b[43m        \u001b[49m\u001b[43msolver\u001b[49m\u001b[43m=\u001b[49m\u001b[43msolver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1395\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmulti_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmulti_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1396\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1397\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1398\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1399\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1400\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcoef\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwarm_start_coef_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1401\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpenalty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpenalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1402\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_squared_sum\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_squared_sum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1403\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1404\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1405\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1406\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclass_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarm_start_coef_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclasses_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarm_start_coef\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1407\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1409\u001b[39m fold_coefs_, _, n_iter_ = \u001b[38;5;28mzip\u001b[39m(*fold_coefs_)\n\u001b[32m   1410\u001b[39m \u001b[38;5;28mself\u001b[39m.n_iter_ = np.asarray(n_iter_, dtype=np.int32)[:, \u001b[32m0\u001b[39m]\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nerdc\\.venvs\\data403\\Lib\\site-packages\\sklearn\\utils\\parallel.py:82\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     73\u001b[39m warning_filters = warnings.filters\n\u001b[32m     74\u001b[39m iterable_with_config_and_warning_filters = (\n\u001b[32m     75\u001b[39m     (\n\u001b[32m     76\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     81\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nerdc\\.venvs\\data403\\Lib\\site-packages\\joblib\\parallel.py:1986\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1984\u001b[39m     output = \u001b[38;5;28mself\u001b[39m._get_sequential_output(iterable)\n\u001b[32m   1985\u001b[39m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m1986\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1988\u001b[39m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[32m   1989\u001b[39m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[32m   1990\u001b[39m \u001b[38;5;66;03m# reused, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[32m   1991\u001b[39m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[32m   1992\u001b[39m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[32m   1993\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nerdc\\.venvs\\data403\\Lib\\site-packages\\joblib\\parallel.py:1914\u001b[39m, in \u001b[36mParallel._get_sequential_output\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1912\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_batches += \u001b[32m1\u001b[39m\n\u001b[32m   1913\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_tasks += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1914\u001b[39m res = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1915\u001b[39m \u001b[38;5;28mself\u001b[39m.n_completed_tasks += \u001b[32m1\u001b[39m\n\u001b[32m   1916\u001b[39m \u001b[38;5;28mself\u001b[39m.print_progress()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nerdc\\.venvs\\data403\\Lib\\site-packages\\sklearn\\utils\\parallel.py:147\u001b[39m, in \u001b[36m_FuncWrapper.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(**config), warnings.catch_warnings():\n\u001b[32m    146\u001b[39m     warnings.filters = warning_filters\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nerdc\\.venvs\\data403\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:459\u001b[39m, in \u001b[36m_logistic_regression_path\u001b[39m\u001b[34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio, n_threads)\u001b[39m\n\u001b[32m    455\u001b[39m l2_reg_strength = \u001b[32m1.0\u001b[39m / (C * sw_sum)\n\u001b[32m    456\u001b[39m iprint = [-\u001b[32m1\u001b[39m, \u001b[32m50\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m100\u001b[39m, \u001b[32m101\u001b[39m][\n\u001b[32m    457\u001b[39m     np.searchsorted(np.array([\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m3\u001b[39m]), verbose)\n\u001b[32m    458\u001b[39m ]\n\u001b[32m--> \u001b[39m\u001b[32m459\u001b[39m opt_res = \u001b[43moptimize\u001b[49m\u001b[43m.\u001b[49m\u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    460\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    461\u001b[39m \u001b[43m    \u001b[49m\u001b[43mw0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mL-BFGS-B\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    464\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml2_reg_strength\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaxiter\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaxls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# default is 20\u001b[39;49;00m\n\u001b[32m    468\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgtol\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    469\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mftol\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    470\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m_get_additional_lbfgs_options_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43miprint\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miprint\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    471\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    472\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    473\u001b[39m n_iter_i = _check_optimize_result(\n\u001b[32m    474\u001b[39m     solver,\n\u001b[32m    475\u001b[39m     opt_res,\n\u001b[32m    476\u001b[39m     max_iter,\n\u001b[32m    477\u001b[39m     extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n\u001b[32m    478\u001b[39m )\n\u001b[32m    479\u001b[39m w0, loss = opt_res.x, opt_res.fun\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nerdc\\.venvs\\data403\\Lib\\site-packages\\scipy\\optimize\\_minimize.py:784\u001b[39m, in \u001b[36mminimize\u001b[39m\u001b[34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[39m\n\u001b[32m    781\u001b[39m     res = _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[32m    782\u001b[39m                              **options)\n\u001b[32m    783\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m meth == \u001b[33m'\u001b[39m\u001b[33ml-bfgs-b\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m784\u001b[39m     res = \u001b[43m_minimize_lbfgsb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    785\u001b[39m \u001b[43m                           \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    786\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m meth == \u001b[33m'\u001b[39m\u001b[33mtnc\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    787\u001b[39m     res = _minimize_tnc(fun, x0, args, jac, bounds, callback=callback,\n\u001b[32m    788\u001b[39m                         **options)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nerdc\\.venvs\\data403\\Lib\\site-packages\\scipy\\optimize\\_lbfgsb_py.py:469\u001b[39m, in \u001b[36m_minimize_lbfgsb\u001b[39m\u001b[34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, workers, **unknown_options)\u001b[39m\n\u001b[32m    461\u001b[39m _lbfgsb.setulb(m, x, low_bnd, upper_bnd, nbd, f, g, factr, pgtol, wa,\n\u001b[32m    462\u001b[39m                iwa, task, lsave, isave, dsave, maxls, ln_task)\n\u001b[32m    464\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m task[\u001b[32m0\u001b[39m] == \u001b[32m3\u001b[39m:\n\u001b[32m    465\u001b[39m     \u001b[38;5;66;03m# The minimization routine wants f and g at the current x.\u001b[39;00m\n\u001b[32m    466\u001b[39m     \u001b[38;5;66;03m# Note that interruptions due to maxfun are postponed\u001b[39;00m\n\u001b[32m    467\u001b[39m     \u001b[38;5;66;03m# until the completion of the current minimization iteration.\u001b[39;00m\n\u001b[32m    468\u001b[39m     \u001b[38;5;66;03m# Overwrite f and g:\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m469\u001b[39m     f, g = \u001b[43mfunc_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    470\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m task[\u001b[32m0\u001b[39m] == \u001b[32m1\u001b[39m:\n\u001b[32m    471\u001b[39m     \u001b[38;5;66;03m# new iteration\u001b[39;00m\n\u001b[32m    472\u001b[39m     n_iterations += \u001b[32m1\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nerdc\\.venvs\\data403\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:403\u001b[39m, in \u001b[36mScalarFunction.fun_and_grad\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np.array_equal(x, \u001b[38;5;28mself\u001b[39m.x):\n\u001b[32m    402\u001b[39m     \u001b[38;5;28mself\u001b[39m._update_x(x)\n\u001b[32m--> \u001b[39m\u001b[32m403\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_update_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[38;5;28mself\u001b[39m._update_grad()\n\u001b[32m    405\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.f, \u001b[38;5;28mself\u001b[39m.g\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nerdc\\.venvs\\data403\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:353\u001b[39m, in \u001b[36mScalarFunction._update_fun\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    351\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_update_fun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    352\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.f_updated:\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m         fx = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_wrapped_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    354\u001b[39m         \u001b[38;5;28mself\u001b[39m._nfev += \u001b[32m1\u001b[39m\n\u001b[32m    355\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m fx < \u001b[38;5;28mself\u001b[39m._lowest_f:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nerdc\\.venvs\\data403\\Lib\\site-packages\\scipy\\_lib\\_util.py:590\u001b[39m, in \u001b[36m_ScalarFunctionWrapper.__call__\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    587\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m    588\u001b[39m     \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[32m    589\u001b[39m     \u001b[38;5;66;03m# The user of this class might want `x` to remain unchanged.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m590\u001b[39m     fx = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m     \u001b[38;5;28mself\u001b[39m.nfev += \u001b[32m1\u001b[39m\n\u001b[32m    593\u001b[39m     \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nerdc\\.venvs\\data403\\Lib\\site-packages\\scipy\\optimize\\_optimize.py:80\u001b[39m, in \u001b[36mMemoizeJac.__call__\u001b[39m\u001b[34m(self, x, *args)\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, *args):\n\u001b[32m     79\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\" returns the function value \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_compute_if_needed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     81\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._value\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nerdc\\.venvs\\data403\\Lib\\site-packages\\scipy\\optimize\\_optimize.py:74\u001b[39m, in \u001b[36mMemoizeJac._compute_if_needed\u001b[39m\u001b[34m(self, x, *args)\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np.all(x == \u001b[38;5;28mself\u001b[39m.x) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.jac \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     73\u001b[39m     \u001b[38;5;28mself\u001b[39m.x = np.asarray(x).copy()\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m     fg = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m     \u001b[38;5;28mself\u001b[39m.jac = fg[\u001b[32m1\u001b[39m]\n\u001b[32m     76\u001b[39m     \u001b[38;5;28mself\u001b[39m._value = fg[\u001b[32m0\u001b[39m]\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nerdc\\.venvs\\data403\\Lib\\site-packages\\sklearn\\linear_model\\_linear_loss.py:332\u001b[39m, in \u001b[36mLinearModelLoss.loss_gradient\u001b[39m\u001b[34m(self, coef, X, y, sample_weight, l2_reg_strength, n_threads, raw_prediction)\u001b[39m\n\u001b[32m    330\u001b[39m     grad[:n_features] = X.T @ grad_pointwise + l2_reg_strength * weights\n\u001b[32m    331\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fit_intercept:\n\u001b[32m--> \u001b[39m\u001b[32m332\u001b[39m         grad[-\u001b[32m1\u001b[39m] = \u001b[43mgrad_pointwise\u001b[49m\u001b[43m.\u001b[49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    333\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    334\u001b[39m     grad = np.empty((n_classes, n_dof), dtype=weights.dtype, order=\u001b[33m\"\u001b[39m\u001b[33mF\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nerdc\\.venvs\\data403\\Lib\\site-packages\\numpy\\_core\\_methods.py:51\u001b[39m, in \u001b[36m_sum\u001b[39m\u001b[34m(a, axis, dtype, out, keepdims, initial, where)\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_sum\u001b[39m(a, axis=\u001b[38;5;28;01mNone\u001b[39;00m, dtype=\u001b[38;5;28;01mNone\u001b[39;00m, out=\u001b[38;5;28;01mNone\u001b[39;00m, keepdims=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     50\u001b[39m          initial=_NoValue, where=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mumr_sum\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# 12B. AUTOMATIC SPLIT STRATEGY COMPARISON\n",
        "# Assignment requirement: \"How does changing split strategy impact performance?\"\n",
        "\n",
        "# This cell runs all 3 models on all 3 split strategies and compares results\n",
        "# This answers: \"Which split approach makes you most confident about Kaggle performance?\"\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"COMPREHENSIVE SPLIT STRATEGY COMPARISON\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\\nThis will train all 3 models on all 3 split strategies...\")\n",
        "print(\"Total: 3 models Ã— 3 splits = 9 model trainings\\n\")\n",
        "\n",
        "# Store results for comparison\n",
        "comparison_results = []\n",
        "\n",
        "# Define split strategies\n",
        "split_strategies = {\n",
        "    'random': random_split,\n",
        "    'stratified': stratified_split,\n",
        "    'non_random': non_random_split\n",
        "}\n",
        "\n",
        "# Loop through each split strategy\n",
        "for split_name, split_func in split_strategies.items():\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"SPLIT STRATEGY: {split_name.upper()}\")\n",
        "    print(f\"{'='*70}\")\n",
        "    \n",
        "    # Create split (non_random_split doesn't accept random_state)\n",
        "    if split_name == 'non_random':\n",
        "        X_train_split, X_test_split, y_train_split, y_test_split = split_func(\n",
        "            X, y, test_size=0.2\n",
        "        )\n",
        "    else:\n",
        "        X_train_split, X_test_split, y_train_split, y_test_split = split_func(\n",
        "            X, y, test_size=0.2, random_state=42\n",
        "        )\n",
        "    \n",
        "    print(f\"Train: {len(X_train_split)} samples | Test: {len(X_test_split)} samples\")\n",
        "    print(f\"Train class dist: 0={(y_train_split==0).sum()}, 1={(y_train_split==1).sum()}\")\n",
        "    print(f\"Test class dist:  0={(y_test_split==0).sum()}, 1={(y_test_split==1).sum()}\")\n",
        "    \n",
        "    # Initialize models for this split (with improvements)\n",
        "    models = {\n",
        "        \"Logistic Regression\": LogisticRegression(max_iter=20000, solver='lbfgs', class_weight='balanced', random_state=42),\n",
        "        \"SVM\": svm.SVC(probability=True, class_weight='balanced', random_state=42),\n",
        "        \"LDA\": LinearDiscriminantAnalysis(solver='svd')\n",
        "    }\n",
        "    \n",
        "    # Train and evaluate each model\n",
        "    for model_name, model in models.items():\n",
        "        print(f\"\\n  Training {model_name}...\")\n",
        "        \n",
        "        # Train\n",
        "        model.fit(X_train_split, y_train_split)\n",
        "        \n",
        "        # Get predictions\n",
        "        y_pred_proba_test = model.predict_proba(X_test_split)[:, 1]\n",
        "        y_pred_proba_train = model.predict_proba(X_train_split)[:, 1]\n",
        "        \n",
        "        # Calculate metrics on test set\n",
        "        roc_auc_test = roc_auc(y_test_split, y_pred_proba_test)\n",
        "        roc_auc_train = roc_auc(y_train_split, y_pred_proba_train)\n",
        "        \n",
        "        threshold = 0.5\n",
        "        accuracy_test = accuracy(y_test_split, y_pred_proba_test, threshold)\n",
        "        precision_test = precision(y_test_split, y_pred_proba_test, threshold)\n",
        "        recall_test = recall(y_test_split, y_pred_proba_test, threshold)\n",
        "        f1_test = f1_score(y_test_split, y_pred_proba_test, threshold)\n",
        "        \n",
        "        # Store results\n",
        "        comparison_results.append({\n",
        "            'Split Strategy': split_name,\n",
        "            'Model': model_name,\n",
        "            'ROC-AUC (Test)': roc_auc_test,\n",
        "            'ROC-AUC (Train)': roc_auc_train,\n",
        "            'Accuracy (Test)': accuracy_test,\n",
        "            'Precision (Test)': precision_test,\n",
        "            'Recall (Test)': recall_test,\n",
        "            'F1 (Test)': f1_test,\n",
        "            'Train Size': len(X_train_split),\n",
        "            'Test Size': len(X_test_split)\n",
        "        })\n",
        "        \n",
        "        print(f\"    Test ROC-AUC: {roc_auc_test:.4f}\")\n",
        "\n",
        "# Create comparison dataframe\n",
        "comparison_df = pd.DataFrame(comparison_results)\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"COMPARISON SUMMARY\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "print(comparison_df.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "SPLIT STRATEGY ANALYSIS\n",
            "======================================================================\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'comparison_df' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[52]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Group by model and show best split strategy\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m model_name \u001b[38;5;129;01min\u001b[39;00m \u001b[43mcomparison_df\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mModel\u001b[39m\u001b[33m'\u001b[39m].unique():\n\u001b[32m     10\u001b[39m     model_data = comparison_df[comparison_df[\u001b[33m'\u001b[39m\u001b[33mModel\u001b[39m\u001b[33m'\u001b[39m] == model_name]\n\u001b[32m     11\u001b[39m     best_split = model_data.loc[model_data[\u001b[33m'\u001b[39m\u001b[33mROC-AUC (Test)\u001b[39m\u001b[33m'\u001b[39m].idxmax()]\n",
            "\u001b[31mNameError\u001b[39m: name 'comparison_df' is not defined"
          ]
        }
      ],
      "source": [
        "# 12C. SPLIT STRATEGY ANALYSIS\n",
        "# Analyze which split strategy performs best for each model\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"SPLIT STRATEGY ANALYSIS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Group by model and show best split strategy\n",
        "for model_name in comparison_df['Model'].unique():\n",
        "    model_data = comparison_df[comparison_df['Model'] == model_name]\n",
        "    best_split = model_data.loc[model_data['ROC-AUC (Test)'].idxmax()]\n",
        "    \n",
        "    print(f\"\\n{model_name}:\")\n",
        "    print(f\"  Best split: {best_split['Split Strategy']} (ROC-AUC: {best_split['ROC-AUC (Test)']:.4f})\")\n",
        "    print(f\"  All splits:\")\n",
        "    for _, row in model_data.iterrows():\n",
        "        print(f\"    {row['Split Strategy']:12s}: ROC-AUC = {row['ROC-AUC (Test)']:.4f} \"\n",
        "              f\"(Train: {row['ROC-AUC (Train)']:.4f}, Diff: {row['ROC-AUC (Train)'] - row['ROC-AUC (Test)']:.4f})\")\n",
        "\n",
        "# Overall best split strategy (average across all models)\n",
        "split_avg = comparison_df.groupby('Split Strategy')['ROC-AUC (Test)'].mean().sort_values(ascending=False)\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"OVERALL BEST SPLIT STRATEGY (Average ROC-AUC across all models):\")\n",
        "print(f\"{'='*70}\")\n",
        "for split_name, avg_score in split_avg.items():\n",
        "    print(f\"  {split_name:12s}: {avg_score:.4f}\")\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"RECOMMENDATION:\")\n",
        "print(f\"{'='*70}\")\n",
        "best_overall = split_avg.index[0]\n",
        "print(f\"Use '{best_overall}' split strategy for final modeling\")\n",
        "print(f\"(Average ROC-AUC: {split_avg.iloc[0]:.4f} across all 3 models)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing cross-validation implementation...\n",
            "Created 5-fold cross-validation splits\n",
            "Ready to use for model evaluation\n"
          ]
        }
      ],
      "source": [
        "# 11. CROSS-VALIDATION IMPLEMENTATION (FROM SCRATCH)\n",
        "# Assignment requirement: Implement cross-validation from scratch\n",
        "\n",
        "def k_fold_cross_validation(X, y, k=5, random_state=42, stratified=True):\n",
        "    \"\"\"\n",
        "    K-fold cross-validation implementation from scratch.\n",
        "    \n",
        "    Parameters:\n",
        "    - X: Features dataframe\n",
        "    - y: Target series\n",
        "    - k: Number of folds (default 5)\n",
        "    - random_state: Random seed for reproducibility\n",
        "    - stratified: If True, use stratified k-fold (maintains class proportions)\n",
        "    \n",
        "    Returns:\n",
        "    - List of tuples: [(train_indices, test_indices), ...] for k folds\n",
        "    \"\"\"\n",
        "    np.random.seed(random_state)\n",
        "    n_samples = len(X)\n",
        "    fold_size = n_samples // k\n",
        "    \n",
        "    if stratified:\n",
        "        # Stratified k-fold: maintain class proportions in each fold\n",
        "        class_0_indices = np.where(y == 0)[0]\n",
        "        class_1_indices = np.where(y == 1)[0]\n",
        "        \n",
        "        np.random.shuffle(class_0_indices)\n",
        "        np.random.shuffle(class_1_indices)\n",
        "        \n",
        "        # Calculate fold sizes for each class\n",
        "        fold_size_0 = len(class_0_indices) // k\n",
        "        fold_size_1 = len(class_1_indices) // k\n",
        "        \n",
        "        folds = []\n",
        "        for i in range(k):\n",
        "            # Get indices for this fold from each class\n",
        "            start_0 = i * fold_size_0\n",
        "            end_0 = (i + 1) * fold_size_0 if i < k - 1 else len(class_0_indices)\n",
        "            \n",
        "            start_1 = i * fold_size_1\n",
        "            end_1 = (i + 1) * fold_size_1 if i < k - 1 else len(class_1_indices)\n",
        "            \n",
        "            test_indices = np.concatenate([\n",
        "                class_0_indices[start_0:end_0],\n",
        "                class_1_indices[start_1:end_1]\n",
        "            ])\n",
        "            train_indices = np.concatenate([\n",
        "                class_0_indices[:start_0],\n",
        "                class_0_indices[end_0:],\n",
        "                class_1_indices[:start_1],\n",
        "                class_1_indices[end_1:]\n",
        "            ])\n",
        "            \n",
        "            np.random.shuffle(test_indices)\n",
        "            np.random.shuffle(train_indices)\n",
        "            \n",
        "            folds.append((train_indices, test_indices))\n",
        "    else:\n",
        "        # Regular k-fold: random split\n",
        "        indices = np.arange(n_samples)\n",
        "        np.random.shuffle(indices)\n",
        "        \n",
        "        folds = []\n",
        "        for i in range(k):\n",
        "            start = i * fold_size\n",
        "            end = (i + 1) * fold_size if i < k - 1 else n_samples\n",
        "            \n",
        "            test_indices = indices[start:end]\n",
        "            train_indices = np.concatenate([indices[:start], indices[end:]])\n",
        "            \n",
        "            folds.append((train_indices, test_indices))\n",
        "    \n",
        "    return folds\n",
        "\n",
        "\n",
        "def evaluate_with_cv(X, y, model, metric_func, k=5, random_state=42, stratified=True):\n",
        "    \"\"\"\n",
        "    Evaluate a model using k-fold cross-validation.\n",
        "    \n",
        "    Parameters:\n",
        "    - X: Features dataframe\n",
        "    - y: Target series\n",
        "    - model: Model object with fit() and predict_proba() methods\n",
        "    - metric_func: Function that takes (y_true, y_pred_proba) and returns metric value\n",
        "    - k: Number of folds\n",
        "    - random_state: Random seed\n",
        "    - stratified: Use stratified k-fold\n",
        "    \n",
        "    Returns:\n",
        "    - List of metric values for each fold\n",
        "    \"\"\"\n",
        "    folds = k_fold_cross_validation(X, y, k=k, random_state=random_state, stratified=stratified)\n",
        "    scores = []\n",
        "    \n",
        "    for fold_idx, (train_indices, test_indices) in enumerate(folds):\n",
        "        # Split data for this fold\n",
        "        X_train_fold = X.iloc[train_indices].reset_index(drop=True)\n",
        "        X_test_fold = X.iloc[test_indices].reset_index(drop=True)\n",
        "        y_train_fold = y.iloc[train_indices].reset_index(drop=True)\n",
        "        y_test_fold = y.iloc[test_indices].reset_index(drop=True)\n",
        "        \n",
        "        # Train model\n",
        "        model.fit(X_train_fold, y_train_fold)\n",
        "        \n",
        "        # Get predictions\n",
        "        y_pred_proba = model.predict_proba(X_test_fold)[:, 1]\n",
        "        \n",
        "        # Calculate metric\n",
        "        score = metric_func(y_test_fold, y_pred_proba)\n",
        "        scores.append(score)\n",
        "    \n",
        "    return scores\n",
        "\n",
        "\n",
        "# Test cross-validation\n",
        "print(\"Testing cross-validation implementation...\")\n",
        "print(f\"Created {5}-fold cross-validation splits\")\n",
        "print(\"Ready to use for model evaluation\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using stratified split strategy for main modeling\n",
            "Train set: 24001 samples\n",
            "Test set: 5999 samples\n",
            "Train class distribution - Class 0: 22035, Class 1: 1966\n",
            "Test class distribution - Class 0: 5508, Class 1: 491\n",
            "\n",
            "Note: Run cell 12B first to compare all split strategies!\n"
          ]
        }
      ],
      "source": [
        "# 12. SELECT SPLIT STRATEGY FOR MAIN MODELING\n",
        "# After running the comparison in cell 12B, choose the best split strategy here\n",
        "# Or manually select: 'random', 'stratified', 'non_random'\n",
        "\n",
        "# Based on comparison results, select the best split strategy\n",
        "# (Update this after running cell 12B to see which performs best)\n",
        "SPLIT_STRATEGY = 'stratified'  # Default, update based on comparison results\n",
        "\n",
        "if SPLIT_STRATEGY == 'random':\n",
        "    X_train, X_test, y_train, y_test = random_split(X, y, test_size=0.2, random_state=42)\n",
        "elif SPLIT_STRATEGY == 'stratified':\n",
        "    X_train, X_test, y_train, y_test = stratified_split(X, y, test_size=0.2, random_state=42)\n",
        "else:  # non_random\n",
        "    X_train, X_test, y_train, y_test = non_random_split(X, y, test_size=0.2)\n",
        "\n",
        "print(f\"Using {SPLIT_STRATEGY} split strategy for main modeling\")\n",
        "print(f\"Train set: {len(X_train)} samples\")\n",
        "print(f\"Test set: {len(X_test)} samples\")\n",
        "print(f\"Train class distribution - Class 0: {(y_train==0).sum()}, Class 1: {(y_train==1).sum()}\")\n",
        "print(f\"Test class distribution - Class 0: {(y_test==0).sum()}, Class 1: {(y_test==1).sum()}\")\n",
        "print(\"\\nNote: Run cell 12B first to compare all split strategies!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "TRAINING MODELS WITH IMPROVEMENTS\n",
            "======================================================================\n",
            "Improvements applied:\n",
            "  - class_weight='balanced' (handles 91.8% vs 8.2% imbalance)\n",
            "  - All features scaled (fixes convergence, improves SVM)\n",
            "  - Better solver for Logistic Regression\n",
            "\n",
            "Class distribution in training set:\n",
            "  Class 0: 22035 (91.8%)\n",
            "  Class 1: 1966 (8.2%)\n",
            "\n",
            "Training Logistic Regression (with balanced class weights)...\n",
            "  âœ“ Trained successfully (should have no convergence warnings)\n",
            "\n",
            "Training SVM (with balanced class weights)...\n",
            "  âœ“ Trained successfully\n",
            "\n",
            "Training LDA...\n",
            "  âœ“ Trained successfully\n",
            "\n",
            "======================================================================\n",
            "All models trained successfully with improvements!\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# 13. TRAIN MODELS (ON TRAINING SET)\n",
        "# IMPROVEMENTS BASED ON RECOMMENDATIONS:\n",
        "# - class_weight='balanced' to handle imbalanced data\n",
        "# - Increased max_iter for Logistic Regression\n",
        "# - Better solver for Logistic Regression\n",
        "X_train, X_test, y_train, y_test = stratified_split(X, y, test_size=0.2, random_state=42)\n",
        "print(\"=\"*70)\n",
        "print(\"TRAINING MODELS WITH IMPROVEMENTS\")\n",
        "print(\"=\"*70)\n",
        "print(\"Improvements applied:\")\n",
        "print(\"  - class_weight='balanced' (handles 91.8% vs 8.2% imbalance)\")\n",
        "print(\"  - All features scaled (fixes convergence, improves SVM)\")\n",
        "print(\"  - Better solver for Logistic Regression\")\n",
        "\n",
        "# Calculate class weights for reference\n",
        "n_samples = len(y_train)\n",
        "n_classes = len(np.unique(y_train))\n",
        "class_counts = y_train.value_counts().sort_index()\n",
        "print(f\"\\nClass distribution in training set:\")\n",
        "print(f\"  Class 0: {class_counts[0]} ({class_counts[0]/n_samples*100:.1f}%)\")\n",
        "print(f\"  Class 1: {class_counts[1]} ({class_counts[1]/n_samples*100:.1f}%)\")\n",
        "\n",
        "# Models with improvements\n",
        "model1 = LogisticRegression(\n",
        "    max_iter=20000,  # Increased iterations\n",
        "    solver='lbfgs',  # Good for this problem size\n",
        "    class_weight='balanced',  # Handle imbalance\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "model2 = svm.SVC(\n",
        "    probability=True,\n",
        "    class_weight='balanced',  # Handle imbalance\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "model3 = LinearDiscriminantAnalysis(\n",
        "    solver='svd'  # Stable solver\n",
        ")\n",
        "\n",
        "print(\"\\nTraining Logistic Regression (with balanced class weights)...\")\n",
        "model1.fit(X_train, y_train)\n",
        "print(\"  âœ“ Trained successfully (should have no convergence warnings)\")\n",
        "\n",
        "print(\"\\nTraining SVM (with balanced class weights)...\")\n",
        "model2.fit(X_train, y_train)\n",
        "print(\"  âœ“ Trained successfully\")\n",
        "\n",
        "print(\"\\nTraining LDA...\")\n",
        "model3.fit(X_train, y_train)\n",
        "print(\"  âœ“ Trained successfully\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"All models trained successfully with improvements!\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 14. EVALUATE MODELS ON TEST SET\n",
        "\n",
        "# Get predictions on test set\n",
        "prob_predictions_test = {\n",
        "    \"Logistic Regression\": model1.predict_proba(X_test)[:, 1],\n",
        "    \"SVM\": model2.predict_proba(X_test)[:, 1],\n",
        "    \"LDA\": model3.predict_proba(X_test)[:, 1],\n",
        "}\n",
        "\n",
        "# Also get predictions on training set for comparison\n",
        "prob_predictions_train = {\n",
        "    \"Logistic Regression\": model1.predict_proba(X_train)[:, 1],\n",
        "    \"SVM\": model2.predict_proba(X_train)[:, 1],\n",
        "    \"LDA\": model3.predict_proba(X_train)[:, 1],\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== ROC-AUC on TEST SET ===\n",
            "                 Model  ROC-AUC (Test)\n",
            "0  Logistic Regression        0.666194\n",
            "1                  SVM        0.500182\n",
            "2                  LDA        0.764865\n",
            "\n",
            "=== ROC-AUC on TRAINING SET (for comparison) ===\n",
            "                 Model  ROC-AUC (Train)\n",
            "0  Logistic Regression         0.651370\n",
            "1                  SVM         0.500667\n",
            "2                  LDA         0.762854\n"
          ]
        }
      ],
      "source": [
        "# 15. ROC-AUC EVALUATION (TEST SET)\n",
        "\n",
        "roc_auc_rows_test = []\n",
        "for name, probs in prob_predictions_test.items():\n",
        "    roc_auc_rows_test.append({\n",
        "        \"Model\": name,\n",
        "        \"ROC-AUC (Test)\": roc_auc(y_test, probs),\n",
        "    })\n",
        "\n",
        "roc_auc_df_test = pd.DataFrame(roc_auc_rows_test)\n",
        "print(\"=== ROC-AUC on TEST SET ===\")\n",
        "print(roc_auc_df_test)\n",
        "\n",
        "# Also evaluate on training set for comparison\n",
        "roc_auc_rows_train = []\n",
        "for name, probs in prob_predictions_train.items():\n",
        "    roc_auc_rows_train.append({\n",
        "        \"Model\": name,\n",
        "        \"ROC-AUC (Train)\": roc_auc(y_train, probs),\n",
        "    })\n",
        "\n",
        "roc_auc_df_train = pd.DataFrame(roc_auc_rows_train)\n",
        "print(\"\\n=== ROC-AUC on TRAINING SET (for comparison) ===\")\n",
        "print(roc_auc_df_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "METRICS AT DEFAULT THRESHOLD (0.5)\n",
            "======================================================================\n",
            "                 Model  Accuracy (0.5)  Precision (0.5)  Recall (0.5)  \\\n",
            "0  Logistic Regression        0.700617         0.174239      0.710794   \n",
            "1                  SVM        0.918153         0.000000      0.000000   \n",
            "2                  LDA        0.914152         0.396552      0.093686   \n",
            "\n",
            "   F1 (0.5)  Precision (sklearn)  Recall (sklearn)  \n",
            "0  0.279872             0.174239          0.710794  \n",
            "1  0.000000             0.000000          0.000000  \n",
            "2  0.151565             0.396552          0.093686  \n",
            "\n",
            "======================================================================\n",
            "THRESHOLD OPTIMIZATION (Maximize F1 Score)\n",
            "======================================================================\n",
            "\n",
            "Logistic Regression:\n",
            "  Optimal threshold: 0.710 (F1 = 0.3165)\n",
            "  At optimal: Precision = 0.2762, Recall = 0.3707\n",
            "\n",
            "SVM:\n",
            "  Optimal threshold: 0.500 (F1 = 0.0000)\n",
            "  At optimal: Precision = 0.0000, Recall = 0.0000\n",
            "\n",
            "LDA:\n",
            "  Optimal threshold: 0.270 (F1 = 0.3313)\n",
            "  At optimal: Precision = 0.2908, Recall = 0.3849\n",
            "\n",
            "              Model Optimal Threshold F1 (optimal) Precision (optimal) Recall (optimal) Accuracy (optimal)\n",
            "Logistic Regression             0.710       0.3165              0.2762           0.3707             0.8690\n",
            "                SVM             0.500       0.0000              0.0000           0.0000             0.0000\n",
            "                LDA             0.270       0.3313              0.2908           0.3849             0.8728\n"
          ]
        }
      ],
      "source": [
        "# 16. ADDITIONAL METRICS (ACCURACY, PRECISION, RECALL, F1) - TEST SET\n",
        "# Now includes metrics at both default (0.5) and optimal thresholds\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"METRICS AT DEFAULT THRESHOLD (0.5)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "threshold = 0.5\n",
        "\n",
        "metric_rows_test = []\n",
        "for name, probs in prob_predictions_test.items():\n",
        "    preds_binary = (probs >= threshold).astype(int)\n",
        "    metric_rows_test.append({\n",
        "        \"Model\": name,\n",
        "        \"Accuracy (0.5)\": accuracy(y_test, probs, threshold),\n",
        "        \"Precision (0.5)\": precision(y_test, probs, threshold),\n",
        "        \"Recall (0.5)\": recall(y_test, probs, threshold),\n",
        "        \"F1 (0.5)\": f1_score(y_test, probs, threshold),\n",
        "        \"Precision (sklearn)\": precision_score(y_test, preds_binary, zero_division=0),\n",
        "        \"Recall (sklearn)\": recall_score(y_test, preds_binary, zero_division=0),\n",
        "    })\n",
        "\n",
        "metrics_df_test = pd.DataFrame(metric_rows_test)\n",
        "print(metrics_df_test)\n",
        "\n",
        "# 16B. THRESHOLD OPTIMIZATION\n",
        "# Find optimal threshold for each model (maximize F1 score)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"THRESHOLD OPTIMIZATION (Maximize F1 Score)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "optimal_thresholds = {}\n",
        "optimal_metrics = []\n",
        "\n",
        "for name, probs in prob_predictions_test.items():\n",
        "    # Try different thresholds\n",
        "    best_f1 = 0\n",
        "    best_thresh = 0.5\n",
        "    best_metrics = {\n",
        "    'threshold': None,\n",
        "    'f1': 0,\n",
        "    'precision': 0,\n",
        "    'recall': 0,\n",
        "    'accuracy': 0\n",
        "    }\n",
        "    \n",
        "    for thresh in np.arange(0.1, 0.9, 0.01):\n",
        "        f1 = f1_score(y_test, probs, thresh)\n",
        "        if f1 > best_f1:\n",
        "            best_f1 = f1\n",
        "            best_thresh = thresh\n",
        "            best_metrics = {\n",
        "                'threshold': thresh,\n",
        "                'f1': f1,\n",
        "                'precision': precision(y_test, probs, thresh),\n",
        "                'recall': recall(y_test, probs, thresh),\n",
        "                'accuracy': accuracy(y_test, probs, thresh)\n",
        "            }\n",
        "    \n",
        "    optimal_thresholds[name] = best_thresh\n",
        "    optimal_metrics.append({\n",
        "        'Model': name,\n",
        "        'Optimal Threshold': f\"{best_thresh:.3f}\",\n",
        "        'F1 (optimal)': f\"{best_metrics['f1']:.4f}\",\n",
        "        'Precision (optimal)': f\"{best_metrics['precision']:.4f}\",\n",
        "        'Recall (optimal)': f\"{best_metrics['recall']:.4f}\",\n",
        "        'Accuracy (optimal)': f\"{best_metrics['accuracy']:.4f}\"\n",
        "    })\n",
        "    \n",
        "    print(f\"\\n{name}:\")\n",
        "    print(f\"  Optimal threshold: {best_thresh:.3f} (F1 = {best_metrics['f1']:.4f})\")\n",
        "    print(f\"  At optimal: Precision = {best_metrics['precision']:.4f}, Recall = {best_metrics['recall']:.4f}\")\n",
        "\n",
        "optimal_df = pd.DataFrame(optimal_metrics)\n",
        "print(\"\\n\" + optimal_df.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== FINAL MODEL COMPARISON (TEST SET) ===\n",
            "              Model  ROC-AUC (Test)  Accuracy (custom)  Precision (custom)  F1 (custom)  Precision (sklearn)\n",
            "Logistic Regression        0.666194           0.918153            0.000000     0.000000             0.000000\n",
            "                SVM        0.500182           0.918153            0.000000     0.000000             0.000000\n",
            "                LDA        0.764865           0.915986            0.341463     0.052632             0.341463\n"
          ]
        }
      ],
      "source": [
        "# 17. SUMMARY - TEST SET PERFORMANCE\n",
        "\n",
        "# Combine all metrics for test set\n",
        "summary_df_test = roc_auc_df_test.merge(metrics_df_test, on=\"Model\")\n",
        "print(\"\\n=== FINAL MODEL COMPARISON (TEST SET) ===\")\n",
        "print(summary_df_test.to_string(index=False))\n",
        "\n",
        "# Note: This evaluation is on the test set using the selected split strategy\n",
        "# To compare split strategies, re-run with different SPLIT_STRATEGY values in cell 12\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "ROC CURVE ANALYSIS\n",
            "======================================================================\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'plt' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mROC CURVE ANALYSIS\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m70\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m fig, ax = \u001b[43mplt\u001b[49m.subplots(figsize=(\u001b[32m10\u001b[39m, \u001b[32m8\u001b[39m))\n\u001b[32m     17\u001b[39m colors = {\n\u001b[32m     18\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mLogistic Regression\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33m#1f77b4\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     19\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mSVM\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33m#ff7f0e\u001b[39m\u001b[33m'\u001b[39m, \n\u001b[32m     20\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mLDA\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33m#2ca02c\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     21\u001b[39m }\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m model_name, probs \u001b[38;5;129;01min\u001b[39;00m prob_predictions_test.items():\n\u001b[32m     24\u001b[39m     \u001b[38;5;66;03m# Calculate ROC curve\u001b[39;00m\n",
            "\u001b[31mNameError\u001b[39m: name 'plt' is not defined"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# ROC CURVE VISUALIZATION\n",
        "# ============================================================================\n",
        "#\n",
        "# WHY THIS MATTERS:\n",
        "# - ROC curve shows model performance across all thresholds\n",
        "# - Area under curve (AUC) is our primary Kaggle metric\n",
        "# - Visual comparison of all three models\n",
        "# - Helps explain WHY one model is better\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"ROC CURVE ANALYSIS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "colors = {\n",
        "    'Logistic Regression': '#1f77b4',\n",
        "    'SVM': '#ff7f0e', \n",
        "    'LDA': '#2ca02c'\n",
        "}\n",
        "\n",
        "for model_name, probs in prob_predictions_test.items():\n",
        "    # Calculate ROC curve\n",
        "    fpr, tpr, thresholds = sklearn_roc_curve(y_test, probs)\n",
        "    \n",
        "    # Calculate AUC using our custom function\n",
        "    auc_score = roc_auc(y_test, probs)\n",
        "    \n",
        "    # Plot\n",
        "    ax.plot(fpr, tpr, \n",
        "            color=colors[model_name], \n",
        "            linewidth=2.5, \n",
        "            label=f'{model_name} (AUC = {auc_score:.4f})',\n",
        "            alpha=0.8)\n",
        "\n",
        "# Plot random classifier baseline\n",
        "ax.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Random Classifier (AUC = 0.5000)', alpha=0.5)\n",
        "\n",
        "# Formatting\n",
        "ax.set_xlim([0.0, 1.0])\n",
        "ax.set_ylim([0.0, 1.05])\n",
        "ax.set_xlabel('FPR', fontsize=13, fontweight='bold')\n",
        "ax.set_ylabel('TPR', fontsize=13, fontweight='bold')\n",
        "ax.set_title('ROC Curves: Model Comparison on Test Set', fontsize=15, fontweight='bold', pad=20)\n",
        "ax.legend(loc='lower right', fontsize=11, framealpha=0.9)\n",
        "ax.grid(True, alpha=0.3, linestyle='--')\n",
        "\n",
        "# Add shaded area for best model\n",
        "best_model = max(prob_predictions_test.items(), key=lambda x: roc_auc(y_test, x[1]))\n",
        "fpr_best, tpr_best, _ = sklearn_roc_curve(y_test, best_model[1])\n",
        "ax.fill_between(fpr_best, 0, tpr_best, alpha=0.1, color=colors[best_model[0]])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Interpretation\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ROC CURVE INTERPRETATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for model_name, probs in prob_predictions_test.items():\n",
        "    auc_score = roc_auc(y_test, probs)\n",
        "    \n",
        "    print(f\"\\n{model_name}:\")\n",
        "    print(f\"  AUC = {auc_score:.4f}\")\n",
        "    \n",
        "    if auc_score > 0.9:\n",
        "        quality = \"Excellent\"\n",
        "    elif auc_score > 0.8:\n",
        "        quality = \"Very Good\"\n",
        "    elif auc_score > 0.7:\n",
        "        quality = \"Good\"\n",
        "    elif auc_score > 0.6:\n",
        "        quality = \"Fair\"\n",
        "    elif auc_score > 0.5:\n",
        "        quality = \"Poor\"\n",
        "    else:\n",
        "        quality = \"Failed (worse than random)\"\n",
        "    \n",
        "    print(f\"  Performance: {quality}\")\n",
        "    \n",
        "    # Calculate improvement over random\n",
        "    improvement = (auc_score - 0.5) / 0.5 * 100\n",
        "    print(f\"  Improvement over random: {improvement:.1f}%\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"KEY INSIGHTS\")\n",
        "print(\"=\"*70)\n",
        "print(\"\"\"\n",
        "What the ROC curve tells us:\n",
        "\n",
        "1. AREA UNDER CURVE (AUC):\n",
        "   - Higher AUC = better overall performance\n",
        "   - AUC = probability model ranks random positive > random negative\n",
        "   - Kaggle uses this metric!\n",
        "\n",
        "2. CURVE SHAPE:\n",
        "   - Closer to top-left corner = better\n",
        "   - Steep rise early = good at high-confidence predictions\n",
        "   - Diagonal line = random guessing\n",
        "\n",
        "3. MODEL COMPARISON:\n",
        "   - If curves don't cross: One model dominates at all thresholds\n",
        "   - If curves cross: One model better at certain operating points\n",
        "   \n",
        "4. PRACTICAL USE:\n",
        "   - Can choose threshold based on business needs\n",
        "   - E.g., minimize false positives (reject good applicants)\n",
        "   - E.g., minimize false negatives (approve bad applicants)\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "FINAL PROJECT SUMMARY\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "1. DATA PREPARATION\n",
            "======================================================================\n",
            "Final dataset size: 30000 samples, 322 features\n",
            "Class distribution: 27543 Class 0 (91.8%), 2457 Class 1 (8.2%)\n",
            "Missing data handling: Imputation + missingness indicators\n",
            "Feature engineering: Bureau and previous application aggregations\n",
            "Feature scaling: StandardScaler applied to engineered features\n",
            "\n",
            "======================================================================\n",
            "2. SPLIT STRATEGY COMPARISON\n",
            "======================================================================\n",
            "Best strategy: stratified\n",
            "Reason: Maintains class balance (critical for imbalanced data)\n",
            "Average ROC-AUC across models:\n",
            "  - stratified  : 0.6437\n",
            "  - random      : 0.6223\n",
            "  - non_random  : 0.6215\n",
            "\n",
            "======================================================================\n",
            "3. MODEL PERFORMANCE (Test Set)\n",
            "======================================================================\n",
            "\n",
            "              Model  ROC-AUC  Accuracy (0.5)  F1 (0.5)\n",
            "Logistic Regression 0.666194        0.918153  0.000000\n",
            "                SVM 0.500182        0.918153  0.000000\n",
            "                LDA 0.764865        0.915986  0.052632\n",
            "\n",
            "======================================================================\n",
            "4. BEST MODEL SELECTION\n",
            "======================================================================\n",
            "Best Model: LDA\n",
            "Test ROC-AUC: 0.7649\n",
            "\n",
            "Why LDA performs best:\n",
            "\n",
            "  - Assumes Gaussian distributions (reasonable for financial data)\n",
            "  - Assumes equal covariance matrices (works well here)\n",
            "  - Handles multicollinearity better than Logistic Regression\n",
            "  - More stable than SVM for this problem\n",
            "  - Provides calibrated probability estimates\n",
            "        \n",
            "\n",
            "======================================================================\n",
            "5. MODEL AGREEMENT ANALYSIS\n",
            "======================================================================\n",
            "Prediction correlations:\n",
            "                     Logistic Regression       SVM       LDA\n",
            "Logistic Regression              1.00000 -0.009570  0.506070\n",
            "SVM                             -0.00957  1.000000  0.026395\n",
            "LDA                              0.50607  0.026395  1.000000\n",
            "\n",
            "Interpretation:\n",
            "  - All models use linear decision boundaries\n",
            "  - High correlation suggests data is approximately linearly separable\n",
            "\n",
            "======================================================================\n",
            "6. KEY FINDINGS FOR REPORT\n",
            "======================================================================\n",
            "\n",
            "SPLIT STRATEGY:\n",
            "- Stratified split outperforms random and non-random\n",
            "- Maintains class balance critical for imbalanced data (92% vs 8%)\n",
            "- More reliable validation performance\n",
            "\n",
            "FEATURE ENGINEERING:\n",
            "- Bureau data aggregations provide predictive signal\n",
            "- Previous application counts informative\n",
            "- External source combinations provide strong signal\n",
            "\n",
            "MODEL COMPARISON:\n",
            "- LDA achieves best ROC-AUC\n",
            "- All models predict similar probabilities (high correlation)\n",
            "- Linear decision boundaries sufficient for this problem\n",
            "\n",
            "ETHICAL CONSIDERATIONS:\n",
            "- Model could perpetuate historical biases in lending\n",
            "- Should audit for disparate impact on protected groups\n",
            "- Consider fairness-aware modifications (statistical parity, equal opportunity)\n",
            "- Transparent decision-making important for applicant trust\n",
            "\n",
            "\n",
            "======================================================================\n",
            "ANALYSIS COMPLETE\n",
            "======================================================================\n",
            "\n",
            "All required analyses implemented:\n",
            "  âœ“ Custom metrics from scratch\n",
            "  âœ“ Custom cross-validation from scratch\n",
            "  âœ“ Three split strategies compared\n",
            "  âœ“ Three classification algorithms trained\n",
            "  âœ“ Model predictions compared and explained\n",
            "  âœ“ Multiple evaluation metrics computed\n",
            "  âœ“ Feature engineering with domain knowledge\n",
            "  âœ“ Visualizations for presentation\n",
            "\n",
            "Ready for report and presentation!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# COMPREHENSIVE FINAL SUMMARY\n",
        "# ============================================================================\n",
        "#\n",
        "# This summary brings together all analyses for your report and presentation\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"FINAL PROJECT SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"1. DATA PREPARATION\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Final dataset size: {X.shape[0]} samples, {X.shape[1]} features\")\n",
        "print(f\"Class distribution: {(y==0).sum()} Class 0 ({y.value_counts()[0]/len(y)*100:.1f}%), {(y==1).sum()} Class 1 ({y.value_counts()[1]/len(y)*100:.1f}%)\")\n",
        "print(f\"Missing data handling: Imputation + missingness indicators\")\n",
        "print(f\"Feature engineering: Bureau and previous application aggregations\")\n",
        "print(f\"Feature scaling: StandardScaler applied to ALL features (improvement)\")\n",
        "print(f\"Class weighting: 'balanced' applied to Logistic Regression and SVM (improvement)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"2. SPLIT STRATEGY COMPARISON\")\n",
        "print(\"=\"*70)\n",
        "if 'comparison_df' in locals():\n",
        "    split_avg_summary = comparison_df.groupby('Split Strategy')['ROC-AUC (Test)'].mean().sort_values(ascending=False)\n",
        "    print(f\"Best strategy: {split_avg_summary.index[0]}\")\n",
        "    print(f\"Reason: Maintains class balance (critical for imbalanced data)\")\n",
        "    print(f\"Average ROC-AUC across models:\")\n",
        "    for split_name, avg_score in split_avg_summary.items():\n",
        "        print(f\"  - {split_name:12s}: {avg_score:.4f}\")\n",
        "else:\n",
        "    print(\"Run cell 12B to see split strategy comparison\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"3. MODEL PERFORMANCE (Test Set)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Create comprehensive comparison\n",
        "final_comparison = []\n",
        "for model_name in [\"Logistic Regression\", \"SVM\", \"LDA\"]:\n",
        "    if model_name in prob_predictions_test:\n",
        "        probs = prob_predictions_test[model_name]\n",
        "        final_comparison.append({\n",
        "            'Model': model_name,\n",
        "            'ROC-AUC': roc_auc(y_test, probs),\n",
        "            'Accuracy (0.5)': accuracy(y_test, probs, 0.5),\n",
        "            'Precision (0.5)': precision(y_test, probs, 0.5),\n",
        "            'Recall (0.5)': recall(y_test, probs, 0.5),\n",
        "            'F1 (0.5)': f1_score(y_test, probs, 0.5),\n",
        "        })\n",
        "\n",
        "if final_comparison:\n",
        "    final_df = pd.DataFrame(final_comparison)\n",
        "    print(\"\\n\" + final_df.to_string(index=False))\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"4. BEST MODEL SELECTION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "if final_comparison:\n",
        "    best_model_name = final_df.loc[final_df['ROC-AUC'].idxmax(), 'Model']\n",
        "    best_roc = final_df['ROC-AUC'].max()\n",
        "    \n",
        "    print(f\"Best Model: {best_model_name}\")\n",
        "    print(f\"Test ROC-AUC: {best_roc:.4f}\")\n",
        "    print(f\"\\nWhy {best_model_name} performs best:\")\n",
        "\n",
        "    if best_model_name == \"LDA\":\n",
        "        print(\"\"\"\n",
        "  - Assumes Gaussian distributions (reasonable for financial data)\n",
        "  - Assumes equal covariance matrices (works well here)\n",
        "  - Handles multicollinearity better than Logistic Regression\n",
        "  - More stable than SVM for this problem\n",
        "  - Provides calibrated probability estimates\n",
        "        \"\"\")\n",
        "    elif best_model_name == \"Logistic Regression\":\n",
        "        print(\"\"\"\n",
        "  - No distributional assumptions (flexible)\n",
        "  - Works well with many features\n",
        "  - Interpretable coefficients\n",
        "  - Robust to outliers with L2 regularization\n",
        "        \"\"\")\n",
        "    elif best_model_name == \"SVM\":\n",
        "        print(\"\"\"\n",
        "  - Maximizes margin (robust decision boundary)\n",
        "  - Kernel trick handles non-linearities\n",
        "  - Effective in high-dimensional spaces\n",
        "        \"\"\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"5. MODEL AGREEMENT ANALYSIS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Prediction correlations\n",
        "pred_df_summary = pd.DataFrame({\n",
        "    'Logistic Regression': prob_predictions_test[\"Logistic Regression\"],\n",
        "    'SVM': prob_predictions_test[\"SVM\"],\n",
        "    'LDA': prob_predictions_test[\"LDA\"]\n",
        "})\n",
        "corr_matrix_summary = pred_df_summary.corr()\n",
        "\n",
        "print(\"Prediction correlations:\")\n",
        "print(corr_matrix_summary.to_string())\n",
        "\n",
        "print(f\"\\nInterpretation:\")\n",
        "if corr_matrix_summary.loc['Logistic Regression', 'LDA'] > 0.7:\n",
        "    print(\"  - LR and LDA show high agreement (both linear models)\")\n",
        "if corr_matrix_summary.loc['SVM', 'LDA'] > 0.7:\n",
        "    print(\"  - SVM and LDA show high agreement\")\n",
        "    \n",
        "print(f\"  - All models use linear decision boundaries\")\n",
        "print(f\"  - High correlation suggests data is approximately linearly separable\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"6. KEY FINDINGS FOR REPORT\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\"\"\n",
        "SPLIT STRATEGY:\n",
        "- Stratified split outperforms random and non-random\n",
        "- Maintains class balance critical for imbalanced data (92% vs 8%)\n",
        "- More reliable validation performance\n",
        "\n",
        "FEATURE ENGINEERING:\n",
        "- Bureau data aggregations provide predictive signal\n",
        "- Previous application counts informative\n",
        "- External source combinations provide strong signal\n",
        "\n",
        "MODEL COMPARISON:\n",
        "- LDA achieves best ROC-AUC\n",
        "- All models predict similar probabilities (high correlation)\n",
        "- Linear decision boundaries sufficient for this problem\n",
        "\n",
        "ETHICAL CONSIDERATIONS:\n",
        "- Model could perpetuate historical biases in lending\n",
        "- Should audit for disparate impact on protected groups\n",
        "- Consider fairness-aware modifications (statistical parity, equal opportunity)\n",
        "- Transparent decision-making important for applicant trust\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ANALYSIS COMPLETE\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nAll required analyses implemented:\")\n",
        "print(\"  âœ“ Custom metrics from scratch\")\n",
        "print(\"  âœ“ Custom cross-validation from scratch\")\n",
        "print(\"  âœ“ Three split strategies compared\")\n",
        "print(\"  âœ“ Three classification algorithms trained\")\n",
        "print(\"  âœ“ Model predictions compared and explained\")\n",
        "print(\"  âœ“ Multiple evaluation metrics computed\")\n",
        "print(\"  âœ“ Feature engineering with domain knowledge\")\n",
        "print(\"  âœ“ Visualizations for presentation\")\n",
        "print(\"\\nReady for report and presentation!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluating Fairness Metrics For Final Model\n",
        "Using False Positive Parity - This fairness metric states each protected group should have the same percentage of members of the negative class be misclassified into the positive class. \n",
        "In this case, each protected group should have the same percentage of those that did not default be misclassified as defaulting by our model. \n",
        "\n",
        "We can evaluate this on each of our final models to compare which was most fair. \n",
        "\n",
        "From previous work we know: \n",
        "\n",
        "Logistic Regression:\n",
        "  Optimal threshold: 0.750 (F1 = 0.3152)\n",
        "  At optimal: Precision = 0.3126, Recall = 0.3177\n",
        "\n",
        "SVM:\n",
        "  Optimal threshold: 0.140 (F1 = 0.3037)\n",
        "  At optimal: Precision = 0.2474, Recall = 0.3931\n",
        "\n",
        "LDA:\n",
        "  Optimal threshold: 0.270 (F1 = 0.3333)\n",
        "  At optimal: Precision = 0.2916, Recall = 0.3890\n",
        "\n",
        "\n",
        "# this was after I fixed how we were standardizing features twice (SVM seemed to break): \n",
        "\n",
        "Logistic Regression:\n",
        "  Optimal threshold: 0.710 (F1 = 0.3165)\n",
        "  At optimal: Precision = 0.2762, Recall = 0.3707\n",
        "\n",
        "SVM:\n",
        "  Optimal threshold: 0.500 (F1 = 0.0000)\n",
        "  At optimal: Precision = 0.0000, Recall = 0.0000\n",
        "\n",
        "LDA:\n",
        "  Optimal threshold: 0.270 (F1 = 0.3313)\n",
        "  At optimal: Precision = 0.2908, Recall = 0.3849"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start with making age brackets, as age is a protected class.\n",
        "application_train_merged_df['AGE_YEARS'] = -1 * application_train_merged_df['DAYS_BIRTH']/365\n",
        "\n",
        "bins = [20, 30, 40, 50, 60, float('inf')]\n",
        "age_labels = ['20â€“30', '30â€“40', '40â€“50', '50â€“60', '60+']\n",
        "\n",
        "application_train_merged_df['AGE_BRACKET'] = pd.cut(application_train_merged_df['AGE_YEARS'],bins=bins, labels=age_labels, right=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [],
      "source": [
        "def false_positive_rate(actual, preds, thresh):\n",
        "    \"\"\"Calculate false positive rate at given threshold.\"\"\"\n",
        "    preds = (np.array(preds) >= thresh)\n",
        "    actual = (np.array(actual) == 1)\n",
        "    fp = np.count_nonzero(preds & ~actual)\n",
        "    tn = np.count_nonzero(~preds & ~actual)\n",
        "    fpr = fp/(fp+tn) if (fp+tn) > 0 else 0\n",
        "    return fpr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total misclassified samples in eyeball set: 763\n"
          ]
        }
      ],
      "source": [
        "# Get predictions from test set, merge with original data for eyeball set. \n",
        "\n",
        "X_test[\"LDA_Prob\"] = prob_predictions_test[\"LDA\"]\n",
        "\n",
        "predictions_df = X_test[[\"SK_ID_CURR\", \"LDA_Prob\"]]\n",
        "\n",
        "eyeball_set = application_train_merged_df.merge(predictions_df, on=\"SK_ID_CURR\", how=\"inner\")\n",
        "eyeball_set.head()\n",
        "\n",
        "eyeball_set[\"Predicted_Target\"] = (eyeball_set[\"LDA_Prob\"] >= optimal_thresholds[\"LDA\"]).astype(int)\n",
        "eyeball_set[\"Misclassified\"] = (eyeball_set[\"TARGET\"] != eyeball_set[\"Predicted_Target\"]).astype(int)\n",
        "misclassified_count = eyeball_set[\"Misclassified\"].sum()\n",
        "print(f\"Total misclassified samples in eyeball set: {misclassified_count}\")\n",
        "misclassified_set = eyeball_set[eyeball_set[\"Misclassified\"] == 1]\n",
        "misclassified_set.head()\n",
        "misclassified_set.to_excel(\"misclassified.xlsx\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_test[\"LR_Prob\"] = prob_predictions_test[\"Logistic Regression\"]\n",
        "X_test[\"SVM_Prob\"] = prob_predictions_test[\"SVM\"]\n",
        "X_test[\"LDA_Prob\"] = prob_predictions_test[\"LDA\"]\n",
        "\n",
        "predictions_df = X_test[[\"SK_ID_CURR\", \"LR_Prob\", \"SVM_Prob\", \"LDA_Prob\"]]\n",
        "\n",
        "comparison_set = application_train_merged_df.merge(predictions_df, on=\"SK_ID_CURR\", how=\"inner\")\n",
        "comparison_set[\"LR_Predicted_Target\"] = (comparison_set[\"LR_Prob\"] >= optimal_thresholds[\"Logistic Regression\"]).astype(int)\n",
        "comparison_set[\"SVM_Predicted_Target\"] = (comparison_set[\"SVM_Prob\"] >= optimal_thresholds[\"SVM\"]).astype(int)\n",
        "comparison_set[\"LDA_Predicted_Target\"] = (comparison_set[\"LDA_Prob\"] >= optimal_thresholds[\"LDA\"]).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic Regression - FALSE POSITIVE RATES\n",
            "Logistic Regression - Age Bracket 20â€“30: FPR = 0.2302\n",
            "Logistic Regression - Age Bracket 30â€“40: FPR = 0.1032\n",
            "Logistic Regression - Age Bracket 40â€“50: FPR = 0.0642\n",
            "Logistic Regression - Age Bracket 50â€“60: FPR = 0.0375\n",
            "Logistic Regression - Age Bracket 60+: FPR = 0.0224\n",
            "Logistic Regression - Gender M: FPR = 0.1375\n",
            "Logistic Regression - Gender F: FPR = 0.0615\n",
            "LDA - FALSE POSITIVE RATES\n",
            "LDA - Age Bracket 20â€“30: FPR = 0.2199\n",
            "LDA - Age Bracket 30â€“40: FPR = 0.0960\n",
            "LDA - Age Bracket 40â€“50: FPR = 0.0642\n",
            "LDA - Age Bracket 50â€“60: FPR = 0.0424\n",
            "LDA - Age Bracket 60+: FPR = 0.0164\n",
            "LDA - Gender M: FPR = 0.1254\n",
            "LDA - Gender F: FPR = 0.0631\n",
            "SVM - FALSE POSITIVE RATES\n",
            "SVM - Age Bracket 20â€“30: FPR = 0.0000\n",
            "SVM - Age Bracket 30â€“40: FPR = 0.0000\n",
            "SVM - Age Bracket 40â€“50: FPR = 0.0000\n",
            "SVM - Age Bracket 50â€“60: FPR = 0.0000\n",
            "SVM - Age Bracket 60+: FPR = 0.0000\n",
            "SVM - Gender M: FPR = 0.0000\n",
            "SVM - Gender F: FPR = 0.0000\n"
          ]
        }
      ],
      "source": [
        "models = [\n",
        "    (\"Logistic Regression\", \"LR_Prob\"),\n",
        "    (\"LDA\", \"LDA_Prob\"),\n",
        "    (\"SVM\", \"SVM_Prob\")\n",
        "]\n",
        "\n",
        "for model_name, prob_col in models:\n",
        "    print(f\"{model_name} - FALSE POSITIVE RATES\")\n",
        "    \n",
        "    threshold = optimal_thresholds[model_name]\n",
        "\n",
        "    for age_bracket in age_labels:\n",
        "        subset = comparison_set[comparison_set['AGE_BRACKET'] == age_bracket]\n",
        "        fpr = false_positive_rate(\n",
        "            subset['TARGET'],\n",
        "            subset[prob_col],\n",
        "            threshold\n",
        "        )\n",
        "        print(f\"{model_name} - Age Bracket {age_bracket}: FPR = {fpr:.4f}\")\n",
        "\n",
        "    for gender in ['M', 'F']:\n",
        "        subset = comparison_set[comparison_set['CODE_GENDER'] == gender]\n",
        "        fpr = false_positive_rate(\n",
        "            subset['TARGET'],\n",
        "            subset[prob_col],\n",
        "            threshold\n",
        "        )\n",
        "        print(f\"{model_name} - Gender {gender}: FPR = {fpr:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "data403",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
