{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Combined Data Pipeline - Cameron H\n",
        "This notebook combines data cleaning, feature engineering, and modeling from codebook.ipynb and data_cleaning.ipynb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# IMPORTS\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import svm\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.metrics import precision_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CUSTOM METRIC FUNCTIONS\n",
        "\n",
        "def roc_auc(actual, preds):\n",
        "    \"\"\"Manual ROC-AUC implementation.\"\"\"\n",
        "    actual = np.array(actual) == 1\n",
        "    tpr = []\n",
        "    fpr = []\n",
        "    for thresh in [x / 100.0 for x in range(0, 101)]:\n",
        "        preds_t = np.array(preds) >= thresh\n",
        "        tp = sum(preds_t & actual)\n",
        "        fp = sum(preds_t & ~actual)\n",
        "        tn = sum(~preds_t & ~actual)\n",
        "        fn = sum(~preds_t & actual)\n",
        "        tpr.append(tp / (tp + fn))\n",
        "        fpr.append(tn / (tn + fp))\n",
        "\n",
        "    auc = 0\n",
        "    for i in range(0, len(tpr) - 1):\n",
        "        auc += ((tpr[i] + tpr[i + 1]) / 2) * (fpr[i + 1] - fpr[i])\n",
        "    return auc\n",
        "\n",
        "\n",
        "def accuracy(actual, preds, thresh):\n",
        "    \"\"\"Calculate accuracy at given threshold.\"\"\"\n",
        "    preds = np.array(preds) >= thresh\n",
        "    actual = np.array(actual) == 1\n",
        "    actual = (actual == 1)\n",
        "    acc = np.count_nonzero(preds == actual)/len(actual)\n",
        "    return acc\n",
        "\n",
        "def precision(actual, preds, thresh):\n",
        "    \"\"\"Calculate precision at given threshold.\"\"\"\n",
        "    preds = (np.array(preds) >= thresh)\n",
        "    actual = (np.array(actual) == 1)\n",
        "    tp = np.count_nonzero(preds & actual)\n",
        "    fp = np.count_nonzero(preds & ~actual)\n",
        "    prec = tp/(tp+fp) if (tp+fp) > 0 else 0\n",
        "    return prec\n",
        "\n",
        "def f1_score(actual, preds, thresh):\n",
        "    \"\"\"Calculate F1 score at given threshold.\"\"\"\n",
        "    preds = (np.array(preds) >= thresh)\n",
        "    actual = (np.array(actual) == 1)\n",
        "    tp = np.count_nonzero(preds & actual)\n",
        "    fp = np.count_nonzero(preds & ~actual)\n",
        "    fn = np.count_nonzero(~preds & actual)\n",
        "    rec = tp/(tp+fn) if (tp+fn) > 0 else 0\n",
        "    prec = tp/(tp+fp) if (tp+fp) > 0 else 0\n",
        "    f1 = (2 * rec * prec)/(rec + prec) if (rec + prec) > 0 else 0\n",
        "    return f1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Application train shape: (307511, 122)\n",
            "Bureau shape: (1716428, 17)\n",
            "Previous application shape: (1670214, 37)\n",
            "\n",
            "Note: Bureau and Previous Application have multiple rows per customer.\n",
            "They will be aggregated (summed/counted) before merging into the main dataset.\n"
          ]
        }
      ],
      "source": [
        "# 1. LOAD DATA\n",
        "# \n",
        "# DATA STRUCTURE EXPLANATION:\n",
        "# - application_train_df: MAIN dataset (1 row per customer/application) - this is our base\n",
        "# - bureau_df: Credit bureau history (MULTIPLE rows per customer - will be aggregated)\n",
        "# - prev_app_df: Previous loan applications (MULTIPLE rows per customer - will be aggregated)\n",
        "#\n",
        "# PROCESS: We'll merge bureau and previous_application data INTO application_train_df\n",
        "#          by aggregating them first (sum/count per customer), then adding as new columns\n",
        "\n",
        "application_train_df = pd.read_csv('../rory_work/application_train.csv')\n",
        "bureau_df = pd.read_csv('../bureau.csv')\n",
        "prev_app_df = pd.read_csv('../previous_application.csv')\n",
        "\n",
        "print(f\"Application train shape: {application_train_df.shape}\")\n",
        "print(f\"Bureau shape: {bureau_df.shape}\")\n",
        "print(f\"Previous application shape: {prev_app_df.shape}\")\n",
        "print(\"\\nNote: Bureau and Previous Application have multiple rows per customer.\")\n",
        "print(\"They will be aggregated (summed/counted) before merging into the main dataset.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Subset application train shape: (30000, 122)\n",
            "Number of unique customers in sample: 30000\n"
          ]
        }
      ],
      "source": [
        "# 2. SUBSET DATA FOR PERFORMANCE\n",
        "# Take a random subset to ensure the notebook can run efficiently\n",
        "# NOTE: Sampling BEFORE aggregation is fine because:\n",
        "# - We'll only merge bureau/prev_app records for customers in our sample (using how='left')\n",
        "# - Aggregation groups by SK_ID_CURR, so each customer gets correct aggregated values\n",
        "# - Customers without bureau/prev_app records will get 0 (filled later)\n",
        "SUBSET_SIZE = 30000\n",
        "application_train_df = application_train_df.sample(n=min(SUBSET_SIZE, len(application_train_df)), random_state=42)\n",
        "\n",
        "# Get list of customer IDs in our sample (for efficiency when merging)\n",
        "sample_customer_ids = set(application_train_df['SK_ID_CURR'].values)\n",
        "\n",
        "print(f\"Subset application train shape: {application_train_df.shape}\")\n",
        "print(f\"Number of unique customers in sample: {len(sample_customer_ids)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Remaining NaNs: 0\n",
            "Shape after cleaning: (30000, 244)\n"
          ]
        }
      ],
      "source": [
        "# 3. DATA CLEANING\n",
        "\n",
        "# DAYS_EMPLOYED = 365243 means \"no employment record\"\n",
        "application_train_df[\"DAYS_EMPLOYED\"] = application_train_df[\"DAYS_EMPLOYED\"].replace(365243, np.nan)\n",
        "\n",
        "# Create missingness indicators (best-practice for this dataset)\n",
        "missing_cols = {}\n",
        "for col in application_train_df.columns:\n",
        "    missing_cols[col + \"_MISSING\"] = application_train_df[col].isna().astype(int)\n",
        "\n",
        "# Add missing indicators efficiently\n",
        "missing_df = pd.DataFrame(missing_cols, index=application_train_df.index)\n",
        "application_train_df = pd.concat([application_train_df, missing_df], axis=1)\n",
        "\n",
        "# Identify categorical columns\n",
        "categorical_cols = application_train_df.select_dtypes(include=[\"object\"]).columns\n",
        "\n",
        "# Coerce numeric-like columns\n",
        "numeric_like_cols = application_train_df.columns.difference(categorical_cols)\n",
        "application_train_df[numeric_like_cols] = application_train_df[numeric_like_cols].apply(\n",
        "    lambda col: pd.to_numeric(col, errors=\"coerce\")\n",
        ")\n",
        "\n",
        "# Set up numeric & categorical column lists\n",
        "numeric_cols = application_train_df.select_dtypes(include=[np.number]).columns\n",
        "numeric_cols = numeric_cols.drop(\"TARGET\")  # do NOT impute target\n",
        "\n",
        "# Impute numerics (median) & categoricals (mode)\n",
        "num_imputer = SimpleImputer(strategy=\"median\")\n",
        "application_train_df[numeric_cols] = num_imputer.fit_transform(application_train_df[numeric_cols])\n",
        "\n",
        "cat_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
        "application_train_df[categorical_cols] = cat_imputer.fit_transform(application_train_df[categorical_cols])\n",
        "\n",
        "# Confirm no NaNs\n",
        "print(\"Remaining NaNs:\", application_train_df.isna().sum().sum())\n",
        "print(f\"Shape after cleaning: {application_train_df.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape after bureau merge: (30000, 248)\n"
          ]
        }
      ],
      "source": [
        "# 4. FEATURE ENGINEERING - BUREAU DATA\n",
        "# \n",
        "# Bureau data has multiple credit records per customer. We aggregate them into summary features:\n",
        "# - Total overdue amount per customer\n",
        "# - Total debt per customer  \n",
        "# - Total times credit was prolonged per customer\n",
        "# - Total days overdue per customer\n",
        "# Then we add these as NEW COLUMNS to the main dataset (one value per customer)\n",
        "\n",
        "# Filter bureau data to only include records for customers in our sample (for efficiency)\n",
        "bureau_filtered = bureau_df[bureau_df['SK_ID_CURR'].isin(sample_customer_ids)].copy()\n",
        "\n",
        "# Merge with bureau data (creates temporary merged df with multiple rows per customer)\n",
        "application_bureau_df = pd.merge(application_train_df, bureau_filtered, on=\"SK_ID_CURR\", how=\"left\")\n",
        "\n",
        "# Aggregate bureau features\n",
        "total_overdue = application_bureau_df.groupby(\"SK_ID_CURR\")[\"AMT_CREDIT_SUM_OVERDUE\"].sum()\n",
        "total_debt = application_bureau_df.groupby(\"SK_ID_CURR\")[\"AMT_CREDIT_SUM_DEBT\"].sum()\n",
        "times_prolonged = application_bureau_df.groupby(\"SK_ID_CURR\")[\"CNT_CREDIT_PROLONG\"].sum()\n",
        "days_overdue = application_bureau_df.groupby(\"SK_ID_CURR\")[\"CREDIT_DAY_OVERDUE\"].sum()\n",
        "\n",
        "# Merge aggregated features back\n",
        "application_train_merged_df = application_train_df.merge(total_overdue, on='SK_ID_CURR', how='left')\n",
        "application_train_merged_df = application_train_merged_df.merge(total_debt, on='SK_ID_CURR', how='left')\n",
        "application_train_merged_df = application_train_merged_df.merge(times_prolonged, on='SK_ID_CURR', how='left')\n",
        "application_train_merged_df = application_train_merged_df.merge(days_overdue, on=\"SK_ID_CURR\", how='left')\n",
        "\n",
        "# Fill NaN values with 0 for new features\n",
        "application_train_merged_df['AMT_CREDIT_SUM_OVERDUE'] = application_train_merged_df['AMT_CREDIT_SUM_OVERDUE'].fillna(0)\n",
        "application_train_merged_df['AMT_CREDIT_SUM_DEBT'] = application_train_merged_df['AMT_CREDIT_SUM_DEBT'].fillna(0)\n",
        "application_train_merged_df['CNT_CREDIT_PROLONG'] = application_train_merged_df['CNT_CREDIT_PROLONG'].fillna(0)\n",
        "application_train_merged_df['CREDIT_DAY_OVERDUE'] = application_train_merged_df['CREDIT_DAY_OVERDUE'].fillna(0)\n",
        "\n",
        "print(f\"Shape after bureau merge: {application_train_merged_df.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape after previous application merge: (30000, 250)\n"
          ]
        }
      ],
      "source": [
        "# 5. FEATURE ENGINEERING - PREVIOUS APPLICATION DATA\n",
        "#\n",
        "# Previous application data has multiple applications per customer. We create summary features:\n",
        "# - PREV_APPS: Count of total previous applications per customer\n",
        "# - NUM_APPROVED: Count of approved previous applications per customer\n",
        "# Then we add these as NEW COLUMNS to the main dataset (one value per customer)\n",
        "\n",
        "# Filter previous application data to only include records for customers in our sample (for efficiency)\n",
        "prev_app_filtered = prev_app_df[prev_app_df['SK_ID_CURR'].isin(sample_customer_ids)].copy()\n",
        "\n",
        "# Count previous applications per customer\n",
        "prev_app_ct = prev_app_filtered[[\"SK_ID_CURR\"]].copy()\n",
        "prev_app_ct[\"PREV_APPS\"] = 1\n",
        "prev_app_ct = prev_app_ct.groupby(\"SK_ID_CURR\")[\"PREV_APPS\"].count().reset_index()\n",
        "\n",
        "# Count approved previous applications\n",
        "if \"NAME_CONTRACT_STATUS\" in prev_app_filtered.columns:\n",
        "    prev_app_approved = prev_app_filtered[[\"SK_ID_CURR\", \"NAME_CONTRACT_STATUS\"]].copy()\n",
        "    prev_app_approved[\"NUM_APPROVED\"] = np.where(prev_app_approved[\"NAME_CONTRACT_STATUS\"] == \"Approved\", 1, 0)\n",
        "    prev_app_approved = prev_app_approved.groupby(\"SK_ID_CURR\")[\"NUM_APPROVED\"].sum().reset_index()\n",
        "    \n",
        "    # Merge counts\n",
        "    prev_app_ct = prev_app_ct.merge(prev_app_approved, on=\"SK_ID_CURR\", how=\"left\")\n",
        "    prev_app_ct[\"NUM_APPROVED\"] = prev_app_ct[\"NUM_APPROVED\"].fillna(0)\n",
        "else:\n",
        "    # If NAME_CONTRACT_STATUS doesn't exist, just use PREV_APPS\n",
        "    prev_app_ct[\"NUM_APPROVED\"] = 0\n",
        "\n",
        "# Merge with main dataframe\n",
        "application_train_merged_df = application_train_merged_df.merge(prev_app_ct, on=\"SK_ID_CURR\", how=\"left\")\n",
        "application_train_merged_df['PREV_APPS'] = application_train_merged_df['PREV_APPS'].fillna(0)\n",
        "application_train_merged_df['NUM_APPROVED'] = application_train_merged_df['NUM_APPROVED'].fillna(0)\n",
        "\n",
        "print(f\"Shape after previous application merge: {application_train_merged_df.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Standardized columns: ['AMT_CREDIT_SUM_OVERDUE', 'AMT_CREDIT_SUM_DEBT', 'CNT_CREDIT_PROLONG', 'CREDIT_DAY_OVERDUE', 'PREV_APPS', 'NUM_APPROVED']\n"
          ]
        }
      ],
      "source": [
        "# 6. STANDARDIZE NEW FEATURES\n",
        "\n",
        "scaler = StandardScaler()\n",
        "cols_to_standardize = ['AMT_CREDIT_SUM_OVERDUE', 'AMT_CREDIT_SUM_DEBT', 'CNT_CREDIT_PROLONG', \n",
        "                       'CREDIT_DAY_OVERDUE', 'PREV_APPS', 'NUM_APPROVED']\n",
        "\n",
        "# Only standardize columns that exist\n",
        "cols_to_standardize = [col for col in cols_to_standardize if col in application_train_merged_df.columns]\n",
        "\n",
        "if cols_to_standardize:\n",
        "    application_train_merged_df[cols_to_standardize] = scaler.fit_transform(application_train_merged_df[cols_to_standardize])\n",
        "    print(f\"Standardized columns: {cols_to_standardize}\")\n",
        "else:\n",
        "    print(\"No columns to standardize\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape after one-hot encoding: (30000, 357)\n"
          ]
        }
      ],
      "source": [
        "# 7. ONE-HOT ENCODE CATEGORICAL FIELDS\n",
        "\n",
        "df_encoded = pd.get_dummies(application_train_merged_df, drop_first=True)\n",
        "print(f\"Shape after one-hot encoding: {df_encoded.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dropped 34 low correlation columns\n",
            "Final shape: (30000, 323)\n"
          ]
        }
      ],
      "source": [
        "# 8. CORRELATION-BASED FILTERING\n",
        "\n",
        "# Only compute correlations on numeric columns\n",
        "numeric_df = df_encoded.select_dtypes(include=[np.number])\n",
        "correlations = numeric_df.corr()[\"TARGET\"]\n",
        "\n",
        "# Filter out low correlation columns\n",
        "low_corr_cols = correlations[abs(correlations) < 0.01].index.tolist()\n",
        "\n",
        "# DO NOT drop TARGET even if correlation calculation returns it\n",
        "low_corr_cols = [col for col in low_corr_cols if col != \"TARGET\"]\n",
        "\n",
        "df_filtered = df_encoded.drop(columns=low_corr_cols, errors='ignore')\n",
        "\n",
        "print(f\"Dropped {len(low_corr_cols)} low correlation columns\")\n",
        "print(f\"Final shape: {df_filtered.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final X shape: (30000, 322)\n",
            "Final y shape: (30000,)\n",
            "Target distribution: {0: 27543, 1: 2457}\n",
            "Target proportion - Class 0: 0.918, Class 1: 0.082\n"
          ]
        }
      ],
      "source": [
        "# 9. PREPARE FOR MODELING\n",
        "\n",
        "# Remove any remaining NaNs\n",
        "df_filtered = df_filtered.dropna()\n",
        "\n",
        "X = df_filtered.drop(\"TARGET\", axis=1)\n",
        "y = df_filtered[\"TARGET\"]\n",
        "\n",
        "print(f\"Final X shape: {X.shape}\")\n",
        "print(f\"Final y shape: {y.shape}\")\n",
        "print(f\"Target distribution: {y.value_counts().to_dict()}\")\n",
        "print(f\"Target proportion - Class 0: {y.value_counts()[0]/len(y):.3f}, Class 1: {y.value_counts()[1]/len(y):.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing split functions...\n",
            "\n",
            "Random Split:\n",
            "  Train: 24000 samples, Class 0: 22007, Class 1: 1993\n",
            "  Test:  6000 samples, Class 0: 5536, Class 1: 464\n",
            "\n",
            "Stratified Split:\n",
            "  Train: 24001 samples, Class 0: 22035, Class 1: 1966\n",
            "  Test:  5999 samples, Class 0: 5508, Class 1: 491\n",
            "\n",
            "Non-Random Split:\n",
            "  Train: 24000 samples, Class 0: 22031, Class 1: 1969\n",
            "  Test:  6000 samples, Class 0: 5512, Class 1: 488\n"
          ]
        }
      ],
      "source": [
        "# 10. DATA SPLITTING FUNCTIONS (FROM SCRATCH)\n",
        "# Assignment requirement: Random split, Stratified split, Non-random split\n",
        "\n",
        "def random_split(X, y, test_size=0.2, random_state=42):\n",
        "    \"\"\"\n",
        "    Random train/test split implementation from scratch.\n",
        "    \n",
        "    Parameters:\n",
        "    - X: Features dataframe\n",
        "    - y: Target series\n",
        "    - test_size: Proportion of data for test set (default 0.2)\n",
        "    - random_state: Random seed for reproducibility\n",
        "    \n",
        "    Returns:\n",
        "    - X_train, X_test, y_train, y_test\n",
        "    \"\"\"\n",
        "    np.random.seed(random_state)\n",
        "    n_samples = len(X)\n",
        "    n_test = int(n_samples * test_size)\n",
        "    \n",
        "    # Create random indices\n",
        "    indices = np.arange(n_samples)\n",
        "    np.random.shuffle(indices)\n",
        "    \n",
        "    # Split indices\n",
        "    test_indices = indices[:n_test]\n",
        "    train_indices = indices[n_test:]\n",
        "    \n",
        "    # Split data\n",
        "    X_train = X.iloc[train_indices].reset_index(drop=True)\n",
        "    X_test = X.iloc[test_indices].reset_index(drop=True)\n",
        "    y_train = y.iloc[train_indices].reset_index(drop=True)\n",
        "    y_test = y.iloc[test_indices].reset_index(drop=True)\n",
        "    \n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "\n",
        "def stratified_split(X, y, test_size=0.2, random_state=42):\n",
        "    \"\"\"\n",
        "    Stratified train/test split implementation from scratch.\n",
        "    Maintains class proportions in both train and test sets.\n",
        "    \n",
        "    Parameters:\n",
        "    - X: Features dataframe\n",
        "    - y: Target series\n",
        "    - test_size: Proportion of data for test set (default 0.2)\n",
        "    - random_state: Random seed for reproducibility\n",
        "    \n",
        "    Returns:\n",
        "    - X_train, X_test, y_train, y_test\n",
        "    \"\"\"\n",
        "    np.random.seed(random_state)\n",
        "    \n",
        "    # Get indices for each class\n",
        "    class_0_indices = np.where(y == 0)[0]\n",
        "    class_1_indices = np.where(y == 1)[0]\n",
        "    \n",
        "    # Shuffle each class's indices\n",
        "    np.random.shuffle(class_0_indices)\n",
        "    np.random.shuffle(class_1_indices)\n",
        "    \n",
        "    # Calculate test size for each class\n",
        "    n_test_0 = int(len(class_0_indices) * test_size)\n",
        "    n_test_1 = int(len(class_1_indices) * test_size)\n",
        "    \n",
        "    # Split indices for each class\n",
        "    test_indices_0 = class_0_indices[:n_test_0]\n",
        "    train_indices_0 = class_0_indices[n_test_0:]\n",
        "    \n",
        "    test_indices_1 = class_1_indices[:n_test_1]\n",
        "    train_indices_1 = class_1_indices[n_test_1:]\n",
        "    \n",
        "    # Combine indices\n",
        "    test_indices = np.concatenate([test_indices_0, test_indices_1])\n",
        "    train_indices = np.concatenate([train_indices_0, train_indices_1])\n",
        "    \n",
        "    # Shuffle combined indices\n",
        "    np.random.shuffle(test_indices)\n",
        "    np.random.shuffle(train_indices)\n",
        "    \n",
        "    # Split data\n",
        "    X_train = X.iloc[train_indices].reset_index(drop=True)\n",
        "    X_test = X.iloc[test_indices].reset_index(drop=True)\n",
        "    y_train = y.iloc[train_indices].reset_index(drop=True)\n",
        "    y_test = y.iloc[test_indices].reset_index(drop=True)\n",
        "    \n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "\n",
        "def non_random_split(X, y, test_size=0.2):\n",
        "    \"\"\"\n",
        "    Non-random train/test split implementation from scratch.\n",
        "    Uses first (1-test_size) for training, last test_size for testing.\n",
        "    This simulates temporal split or preserves data order.\n",
        "    \n",
        "    Parameters:\n",
        "    - X: Features dataframe\n",
        "    - y: Target series\n",
        "    - test_size: Proportion of data for test set (default 0.2)\n",
        "    \n",
        "    Returns:\n",
        "    - X_train, X_test, y_train, y_test\n",
        "    \"\"\"\n",
        "    n_samples = len(X)\n",
        "    n_test = int(n_samples * test_size)\n",
        "    n_train = n_samples - n_test\n",
        "    \n",
        "    # Split by order (first n_train for train, last n_test for test)\n",
        "    train_indices = np.arange(n_train)\n",
        "    test_indices = np.arange(n_train, n_samples)\n",
        "    \n",
        "    # Split data\n",
        "    X_train = X.iloc[train_indices].reset_index(drop=True)\n",
        "    X_test = X.iloc[test_indices].reset_index(drop=True)\n",
        "    y_train = y.iloc[train_indices].reset_index(drop=True)\n",
        "    y_test = y.iloc[test_indices].reset_index(drop=True)\n",
        "    \n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "\n",
        "# Test the split functions\n",
        "print(\"Testing split functions...\")\n",
        "X_train_rand, X_test_rand, y_train_rand, y_test_rand = random_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train_strat, X_test_strat, y_train_strat, y_test_strat = stratified_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train_nonrand, X_test_nonrand, y_train_nonrand, y_test_nonrand = non_random_split(X, y, test_size=0.2)\n",
        "\n",
        "print(f\"\\nRandom Split:\")\n",
        "print(f\"  Train: {len(X_train_rand)} samples, Class 0: {(y_train_rand==0).sum()}, Class 1: {(y_train_rand==1).sum()}\")\n",
        "print(f\"  Test:  {len(X_test_rand)} samples, Class 0: {(y_test_rand==0).sum()}, Class 1: {(y_test_rand==1).sum()}\")\n",
        "\n",
        "print(f\"\\nStratified Split:\")\n",
        "print(f\"  Train: {len(X_train_strat)} samples, Class 0: {(y_train_strat==0).sum()}, Class 1: {(y_train_strat==1).sum()}\")\n",
        "print(f\"  Test:  {len(X_test_strat)} samples, Class 0: {(y_test_strat==0).sum()}, Class 1: {(y_test_strat==1).sum()}\")\n",
        "\n",
        "print(f\"\\nNon-Random Split:\")\n",
        "print(f\"  Train: {len(X_train_nonrand)} samples, Class 0: {(y_train_nonrand==0).sum()}, Class 1: {(y_train_nonrand==1).sum()}\")\n",
        "print(f\"  Test:  {len(X_test_nonrand)} samples, Class 0: {(y_test_nonrand==0).sum()}, Class 1: {(y_test_nonrand==1).sum()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "COMPREHENSIVE SPLIT STRATEGY COMPARISON\n",
            "======================================================================\n",
            "\n",
            "This will train all 3 models on all 3 split strategies...\n",
            "Total: 3 models × 3 splits = 9 model trainings\n",
            "\n",
            "\n",
            "======================================================================\n",
            "SPLIT STRATEGY: RANDOM\n",
            "======================================================================\n",
            "Train: 24000 samples | Test: 6000 samples\n",
            "Train class dist: 0=22007, 1=1993\n",
            "Test class dist:  0=5536, 1=464\n",
            "\n",
            "  Training Logistic Regression...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Panda\\OneDrive\\Desktop\\Data 403 - Project 2\\DATA403-Project-2\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 10000 iteration(s) (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
            "\n",
            "Increase the number of iterations to improve the convergence (max_iter=10000).\n",
            "You might also want to scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Test ROC-AUC: 0.6209\n",
            "\n",
            "  Training SVM...\n",
            "    Test ROC-AUC: 0.4999\n",
            "\n",
            "  Training LDA...\n",
            "    Test ROC-AUC: 0.7460\n",
            "\n",
            "======================================================================\n",
            "SPLIT STRATEGY: STRATIFIED\n",
            "======================================================================\n",
            "Train: 24001 samples | Test: 5999 samples\n",
            "Train class dist: 0=22035, 1=1966\n",
            "Test class dist:  0=5508, 1=491\n",
            "\n",
            "  Training Logistic Regression...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Panda\\OneDrive\\Desktop\\Data 403 - Project 2\\DATA403-Project-2\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 10000 iteration(s) (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
            "\n",
            "Increase the number of iterations to improve the convergence (max_iter=10000).\n",
            "You might also want to scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Test ROC-AUC: 0.6662\n",
            "\n",
            "  Training SVM...\n",
            "    Test ROC-AUC: 0.5002\n",
            "\n",
            "  Training LDA...\n",
            "    Test ROC-AUC: 0.7649\n",
            "\n",
            "======================================================================\n",
            "SPLIT STRATEGY: NON_RANDOM\n",
            "======================================================================\n",
            "Train: 24000 samples | Test: 6000 samples\n",
            "Train class dist: 0=22031, 1=1969\n",
            "Test class dist:  0=5512, 1=488\n",
            "\n",
            "  Training Logistic Regression...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Panda\\OneDrive\\Desktop\\Data 403 - Project 2\\DATA403-Project-2\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 10000 iteration(s) (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
            "\n",
            "Increase the number of iterations to improve the convergence (max_iter=10000).\n",
            "You might also want to scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Test ROC-AUC: 0.6286\n",
            "\n",
            "  Training SVM...\n",
            "    Test ROC-AUC: 0.5003\n",
            "\n",
            "  Training LDA...\n",
            "    Test ROC-AUC: 0.7355\n",
            "\n",
            "======================================================================\n",
            "COMPARISON SUMMARY\n",
            "======================================================================\n",
            "\n",
            "Split Strategy               Model  ROC-AUC (Test)  ROC-AUC (Train)  Accuracy (Test)  Precision (Test)  F1 (Test)  Train Size  Test Size\n",
            "        random Logistic Regression        0.620860         0.648513         0.922667          0.000000   0.000000       24000       6000\n",
            "        random                 SVM        0.499910         0.500319         0.922667          0.000000   0.000000       24000       6000\n",
            "        random                 LDA        0.746042         0.767503         0.921333          0.409091   0.070866       24000       6000\n",
            "    stratified Logistic Regression        0.666194         0.651370         0.918153          0.000000   0.000000       24001       5999\n",
            "    stratified                 SVM        0.500182         0.500667         0.918153          0.000000   0.000000       24001       5999\n",
            "    stratified                 LDA        0.764865         0.762854         0.915986          0.341463   0.052632       24001       5999\n",
            "    non_random Logistic Regression        0.628603         0.648892         0.918333          0.000000   0.000000       24000       6000\n",
            "    non_random                 SVM        0.500272         0.500780         0.918667          0.000000   0.000000       24000       6000\n",
            "    non_random                 LDA        0.735498         0.769387         0.916667          0.350000   0.053030       24000       6000\n"
          ]
        }
      ],
      "source": [
        "# 12B. AUTOMATIC SPLIT STRATEGY COMPARISON\n",
        "# Assignment requirement: \"How does changing split strategy impact performance?\"\n",
        "\n",
        "# This cell runs all 3 models on all 3 split strategies and compares results\n",
        "# This answers: \"Which split approach makes you most confident about Kaggle performance?\"\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"COMPREHENSIVE SPLIT STRATEGY COMPARISON\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\\nThis will train all 3 models on all 3 split strategies...\")\n",
        "print(\"Total: 3 models × 3 splits = 9 model trainings\\n\")\n",
        "\n",
        "# Store results for comparison\n",
        "comparison_results = []\n",
        "\n",
        "# Define split strategies\n",
        "split_strategies = {\n",
        "    'random': random_split,\n",
        "    'stratified': stratified_split,\n",
        "    'non_random': non_random_split\n",
        "}\n",
        "\n",
        "# Loop through each split strategy\n",
        "for split_name, split_func in split_strategies.items():\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"SPLIT STRATEGY: {split_name.upper()}\")\n",
        "    print(f\"{'='*70}\")\n",
        "    \n",
        "    # Create split (non_random_split doesn't accept random_state)\n",
        "    if split_name == 'non_random':\n",
        "        X_train_split, X_test_split, y_train_split, y_test_split = split_func(\n",
        "            X, y, test_size=0.2\n",
        "        )\n",
        "    else:\n",
        "        X_train_split, X_test_split, y_train_split, y_test_split = split_func(\n",
        "            X, y, test_size=0.2, random_state=42\n",
        "        )\n",
        "    \n",
        "    print(f\"Train: {len(X_train_split)} samples | Test: {len(X_test_split)} samples\")\n",
        "    print(f\"Train class dist: 0={(y_train_split==0).sum()}, 1={(y_train_split==1).sum()}\")\n",
        "    print(f\"Test class dist:  0={(y_test_split==0).sum()}, 1={(y_test_split==1).sum()}\")\n",
        "    \n",
        "    # Initialize models for this split\n",
        "    models = {\n",
        "        \"Logistic Regression\": LogisticRegression(max_iter=10000),\n",
        "        \"SVM\": svm.SVC(probability=True),\n",
        "        \"LDA\": LinearDiscriminantAnalysis()\n",
        "    }\n",
        "    \n",
        "    # Train and evaluate each model\n",
        "    for model_name, model in models.items():\n",
        "        print(f\"\\n  Training {model_name}...\")\n",
        "        \n",
        "        # Train\n",
        "        model.fit(X_train_split, y_train_split)\n",
        "        \n",
        "        # Get predictions\n",
        "        y_pred_proba_test = model.predict_proba(X_test_split)[:, 1]\n",
        "        y_pred_proba_train = model.predict_proba(X_train_split)[:, 1]\n",
        "        \n",
        "        # Calculate metrics on test set\n",
        "        roc_auc_test = roc_auc(y_test_split, y_pred_proba_test)\n",
        "        roc_auc_train = roc_auc(y_train_split, y_pred_proba_train)\n",
        "        \n",
        "        threshold = 0.5\n",
        "        accuracy_test = accuracy(y_test_split, y_pred_proba_test, threshold)\n",
        "        precision_test = precision(y_test_split, y_pred_proba_test, threshold)\n",
        "        f1_test = f1_score(y_test_split, y_pred_proba_test, threshold)\n",
        "        \n",
        "        # Store results\n",
        "        comparison_results.append({\n",
        "            'Split Strategy': split_name,\n",
        "            'Model': model_name,\n",
        "            'ROC-AUC (Test)': roc_auc_test,\n",
        "            'ROC-AUC (Train)': roc_auc_train,\n",
        "            'Accuracy (Test)': accuracy_test,\n",
        "            'Precision (Test)': precision_test,\n",
        "            'F1 (Test)': f1_test,\n",
        "            'Train Size': len(X_train_split),\n",
        "            'Test Size': len(X_test_split)\n",
        "        })\n",
        "        \n",
        "        print(f\"    Test ROC-AUC: {roc_auc_test:.4f}\")\n",
        "\n",
        "# Create comparison dataframe\n",
        "comparison_df = pd.DataFrame(comparison_results)\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"COMPARISON SUMMARY\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "print(comparison_df.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "SPLIT STRATEGY ANALYSIS\n",
            "======================================================================\n",
            "\n",
            "Logistic Regression:\n",
            "  Best split: stratified (ROC-AUC: 0.6662)\n",
            "  All splits:\n",
            "    random      : ROC-AUC = 0.6209 (Train: 0.6485, Diff: 0.0277)\n",
            "    stratified  : ROC-AUC = 0.6662 (Train: 0.6514, Diff: -0.0148)\n",
            "    non_random  : ROC-AUC = 0.6286 (Train: 0.6489, Diff: 0.0203)\n",
            "\n",
            "SVM:\n",
            "  Best split: non_random (ROC-AUC: 0.5003)\n",
            "  All splits:\n",
            "    random      : ROC-AUC = 0.4999 (Train: 0.5003, Diff: 0.0004)\n",
            "    stratified  : ROC-AUC = 0.5002 (Train: 0.5007, Diff: 0.0005)\n",
            "    non_random  : ROC-AUC = 0.5003 (Train: 0.5008, Diff: 0.0005)\n",
            "\n",
            "LDA:\n",
            "  Best split: stratified (ROC-AUC: 0.7649)\n",
            "  All splits:\n",
            "    random      : ROC-AUC = 0.7460 (Train: 0.7675, Diff: 0.0215)\n",
            "    stratified  : ROC-AUC = 0.7649 (Train: 0.7629, Diff: -0.0020)\n",
            "    non_random  : ROC-AUC = 0.7355 (Train: 0.7694, Diff: 0.0339)\n",
            "\n",
            "======================================================================\n",
            "OVERALL BEST SPLIT STRATEGY (Average ROC-AUC across all models):\n",
            "======================================================================\n",
            "  stratified  : 0.6437\n",
            "  random      : 0.6223\n",
            "  non_random  : 0.6215\n",
            "\n",
            "======================================================================\n",
            "RECOMMENDATION:\n",
            "======================================================================\n",
            "Use 'stratified' split strategy for final modeling\n",
            "(Average ROC-AUC: 0.6437 across all 3 models)\n"
          ]
        }
      ],
      "source": [
        "# 12C. SPLIT STRATEGY ANALYSIS\n",
        "# Analyze which split strategy performs best for each model\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"SPLIT STRATEGY ANALYSIS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Group by model and show best split strategy\n",
        "for model_name in comparison_df['Model'].unique():\n",
        "    model_data = comparison_df[comparison_df['Model'] == model_name]\n",
        "    best_split = model_data.loc[model_data['ROC-AUC (Test)'].idxmax()]\n",
        "    \n",
        "    print(f\"\\n{model_name}:\")\n",
        "    print(f\"  Best split: {best_split['Split Strategy']} (ROC-AUC: {best_split['ROC-AUC (Test)']:.4f})\")\n",
        "    print(f\"  All splits:\")\n",
        "    for _, row in model_data.iterrows():\n",
        "        print(f\"    {row['Split Strategy']:12s}: ROC-AUC = {row['ROC-AUC (Test)']:.4f} \"\n",
        "              f\"(Train: {row['ROC-AUC (Train)']:.4f}, Diff: {row['ROC-AUC (Train)'] - row['ROC-AUC (Test)']:.4f})\")\n",
        "\n",
        "# Overall best split strategy (average across all models)\n",
        "split_avg = comparison_df.groupby('Split Strategy')['ROC-AUC (Test)'].mean().sort_values(ascending=False)\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"OVERALL BEST SPLIT STRATEGY (Average ROC-AUC across all models):\")\n",
        "print(f\"{'='*70}\")\n",
        "for split_name, avg_score in split_avg.items():\n",
        "    print(f\"  {split_name:12s}: {avg_score:.4f}\")\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"RECOMMENDATION:\")\n",
        "print(f\"{'='*70}\")\n",
        "best_overall = split_avg.index[0]\n",
        "print(f\"Use '{best_overall}' split strategy for final modeling\")\n",
        "print(f\"(Average ROC-AUC: {split_avg.iloc[0]:.4f} across all 3 models)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing cross-validation implementation...\n",
            "Created 5-fold cross-validation splits\n",
            "Ready to use for model evaluation\n"
          ]
        }
      ],
      "source": [
        "# 11. CROSS-VALIDATION IMPLEMENTATION (FROM SCRATCH)\n",
        "# Assignment requirement: Implement cross-validation from scratch\n",
        "\n",
        "def k_fold_cross_validation(X, y, k=5, random_state=42, stratified=True):\n",
        "    \"\"\"\n",
        "    K-fold cross-validation implementation from scratch.\n",
        "    \n",
        "    Parameters:\n",
        "    - X: Features dataframe\n",
        "    - y: Target series\n",
        "    - k: Number of folds (default 5)\n",
        "    - random_state: Random seed for reproducibility\n",
        "    - stratified: If True, use stratified k-fold (maintains class proportions)\n",
        "    \n",
        "    Returns:\n",
        "    - List of tuples: [(train_indices, test_indices), ...] for k folds\n",
        "    \"\"\"\n",
        "    np.random.seed(random_state)\n",
        "    n_samples = len(X)\n",
        "    fold_size = n_samples // k\n",
        "    \n",
        "    if stratified:\n",
        "        # Stratified k-fold: maintain class proportions in each fold\n",
        "        class_0_indices = np.where(y == 0)[0]\n",
        "        class_1_indices = np.where(y == 1)[0]\n",
        "        \n",
        "        np.random.shuffle(class_0_indices)\n",
        "        np.random.shuffle(class_1_indices)\n",
        "        \n",
        "        # Calculate fold sizes for each class\n",
        "        fold_size_0 = len(class_0_indices) // k\n",
        "        fold_size_1 = len(class_1_indices) // k\n",
        "        \n",
        "        folds = []\n",
        "        for i in range(k):\n",
        "            # Get indices for this fold from each class\n",
        "            start_0 = i * fold_size_0\n",
        "            end_0 = (i + 1) * fold_size_0 if i < k - 1 else len(class_0_indices)\n",
        "            \n",
        "            start_1 = i * fold_size_1\n",
        "            end_1 = (i + 1) * fold_size_1 if i < k - 1 else len(class_1_indices)\n",
        "            \n",
        "            test_indices = np.concatenate([\n",
        "                class_0_indices[start_0:end_0],\n",
        "                class_1_indices[start_1:end_1]\n",
        "            ])\n",
        "            train_indices = np.concatenate([\n",
        "                class_0_indices[:start_0],\n",
        "                class_0_indices[end_0:],\n",
        "                class_1_indices[:start_1],\n",
        "                class_1_indices[end_1:]\n",
        "            ])\n",
        "            \n",
        "            np.random.shuffle(test_indices)\n",
        "            np.random.shuffle(train_indices)\n",
        "            \n",
        "            folds.append((train_indices, test_indices))\n",
        "    else:\n",
        "        # Regular k-fold: random split\n",
        "        indices = np.arange(n_samples)\n",
        "        np.random.shuffle(indices)\n",
        "        \n",
        "        folds = []\n",
        "        for i in range(k):\n",
        "            start = i * fold_size\n",
        "            end = (i + 1) * fold_size if i < k - 1 else n_samples\n",
        "            \n",
        "            test_indices = indices[start:end]\n",
        "            train_indices = np.concatenate([indices[:start], indices[end:]])\n",
        "            \n",
        "            folds.append((train_indices, test_indices))\n",
        "    \n",
        "    return folds\n",
        "\n",
        "\n",
        "def evaluate_with_cv(X, y, model, metric_func, k=5, random_state=42, stratified=True):\n",
        "    \"\"\"\n",
        "    Evaluate a model using k-fold cross-validation.\n",
        "    \n",
        "    Parameters:\n",
        "    - X: Features dataframe\n",
        "    - y: Target series\n",
        "    - model: Model object with fit() and predict_proba() methods\n",
        "    - metric_func: Function that takes (y_true, y_pred_proba) and returns metric value\n",
        "    - k: Number of folds\n",
        "    - random_state: Random seed\n",
        "    - stratified: Use stratified k-fold\n",
        "    \n",
        "    Returns:\n",
        "    - List of metric values for each fold\n",
        "    \"\"\"\n",
        "    folds = k_fold_cross_validation(X, y, k=k, random_state=random_state, stratified=stratified)\n",
        "    scores = []\n",
        "    \n",
        "    for fold_idx, (train_indices, test_indices) in enumerate(folds):\n",
        "        # Split data for this fold\n",
        "        X_train_fold = X.iloc[train_indices].reset_index(drop=True)\n",
        "        X_test_fold = X.iloc[test_indices].reset_index(drop=True)\n",
        "        y_train_fold = y.iloc[train_indices].reset_index(drop=True)\n",
        "        y_test_fold = y.iloc[test_indices].reset_index(drop=True)\n",
        "        \n",
        "        # Train model\n",
        "        model.fit(X_train_fold, y_train_fold)\n",
        "        \n",
        "        # Get predictions\n",
        "        y_pred_proba = model.predict_proba(X_test_fold)[:, 1]\n",
        "        \n",
        "        # Calculate metric\n",
        "        score = metric_func(y_test_fold, y_pred_proba)\n",
        "        scores.append(score)\n",
        "    \n",
        "    return scores\n",
        "\n",
        "\n",
        "# Test cross-validation\n",
        "print(\"Testing cross-validation implementation...\")\n",
        "print(f\"Created {5}-fold cross-validation splits\")\n",
        "print(\"Ready to use for model evaluation\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using stratified split strategy\n",
            "Train set: 24001 samples\n",
            "Test set: 5999 samples\n",
            "Train class distribution - Class 0: 22035, Class 1: 1966\n",
            "Test class distribution - Class 0: 5508, Class 1: 491\n"
          ]
        }
      ],
      "source": [
        "# 12. COMPARE SPLIT STRATEGIES\n",
        "# Assignment requirement: \"How does changing split strategy impact performance?\"\n",
        "\n",
        "# We'll train models on each split and compare validation performance\n",
        "# For now, we'll use one split strategy for the main modeling\n",
        "# Full comparison will be done after models are trained\n",
        "\n",
        "# Choose which split to use for main modeling\n",
        "# Options: 'random', 'stratified', 'non_random'\n",
        "SPLIT_STRATEGY = 'stratified'  # Can change this to compare\n",
        "\n",
        "if SPLIT_STRATEGY == 'random':\n",
        "    X_train, X_test, y_train, y_test = random_split(X, y, test_size=0.2, random_state=42)\n",
        "elif SPLIT_STRATEGY == 'stratified':\n",
        "    X_train, X_test, y_train, y_test = stratified_split(X, y, test_size=0.2, random_state=42)\n",
        "else:  # non_random\n",
        "    X_train, X_test, y_train, y_test = non_random_split(X, y, test_size=0.2)\n",
        "\n",
        "print(f\"Using {SPLIT_STRATEGY} split strategy\")\n",
        "print(f\"Train set: {len(X_train)} samples\")\n",
        "print(f\"Test set: {len(X_test)} samples\")\n",
        "print(f\"Train class distribution - Class 0: {(y_train==0).sum()}, Class 1: {(y_train==1).sum()}\")\n",
        "print(f\"Test class distribution - Class 0: {(y_test==0).sum()}, Class 1: {(y_test==1).sum()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Logistic Regression...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Panda\\OneDrive\\Desktop\\Data 403 - Project 2\\DATA403-Project-2\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 10000 iteration(s) (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
            "\n",
            "Increase the number of iterations to improve the convergence (max_iter=10000).\n",
            "You might also want to scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training SVM...\n",
            "Training LDA...\n",
            "All models trained successfully on training set!\n"
          ]
        }
      ],
      "source": [
        "# 13. TRAIN MODELS (ON TRAINING SET)\n",
        "\n",
        "model1 = LogisticRegression(max_iter=10000)\n",
        "model2 = svm.SVC(probability=True)\n",
        "model3 = LinearDiscriminantAnalysis()\n",
        "\n",
        "print(\"Training Logistic Regression...\")\n",
        "model1.fit(X_train, y_train)\n",
        "\n",
        "print(\"Training SVM...\")\n",
        "model2.fit(X_train, y_train)\n",
        "\n",
        "print(\"Training LDA...\")\n",
        "model3.fit(X_train, y_train)\n",
        "\n",
        "print(\"All models trained successfully on training set!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 14. EVALUATE MODELS ON TEST SET\n",
        "\n",
        "# Get predictions on test set\n",
        "prob_predictions_test = {\n",
        "    \"Logistic Regression\": model1.predict_proba(X_test)[:, 1],\n",
        "    \"SVM\": model2.predict_proba(X_test)[:, 1],\n",
        "    \"LDA\": model3.predict_proba(X_test)[:, 1],\n",
        "}\n",
        "\n",
        "# Also get predictions on training set for comparison\n",
        "prob_predictions_train = {\n",
        "    \"Logistic Regression\": model1.predict_proba(X_train)[:, 1],\n",
        "    \"SVM\": model2.predict_proba(X_train)[:, 1],\n",
        "    \"LDA\": model3.predict_proba(X_train)[:, 1],\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== ROC-AUC on TEST SET ===\n",
            "                 Model  ROC-AUC (Test)\n",
            "0  Logistic Regression        0.666194\n",
            "1                  SVM        0.500182\n",
            "2                  LDA        0.764865\n",
            "\n",
            "=== ROC-AUC on TRAINING SET (for comparison) ===\n",
            "                 Model  ROC-AUC (Train)\n",
            "0  Logistic Regression         0.651370\n",
            "1                  SVM         0.500667\n",
            "2                  LDA         0.762854\n"
          ]
        }
      ],
      "source": [
        "# 15. ROC-AUC EVALUATION (TEST SET)\n",
        "\n",
        "roc_auc_rows_test = []\n",
        "for name, probs in prob_predictions_test.items():\n",
        "    roc_auc_rows_test.append({\n",
        "        \"Model\": name,\n",
        "        \"ROC-AUC (Test)\": roc_auc(y_test, probs),\n",
        "    })\n",
        "\n",
        "roc_auc_df_test = pd.DataFrame(roc_auc_rows_test)\n",
        "print(\"=== ROC-AUC on TEST SET ===\")\n",
        "print(roc_auc_df_test)\n",
        "\n",
        "# Also evaluate on training set for comparison\n",
        "roc_auc_rows_train = []\n",
        "for name, probs in prob_predictions_train.items():\n",
        "    roc_auc_rows_train.append({\n",
        "        \"Model\": name,\n",
        "        \"ROC-AUC (Train)\": roc_auc(y_train, probs),\n",
        "    })\n",
        "\n",
        "roc_auc_df_train = pd.DataFrame(roc_auc_rows_train)\n",
        "print(\"\\n=== ROC-AUC on TRAINING SET (for comparison) ===\")\n",
        "print(roc_auc_df_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== METRICS on TEST SET ===\n",
            "                 Model  Accuracy (custom)  Precision (custom)  F1 (custom)  \\\n",
            "0  Logistic Regression           0.918153            0.000000     0.000000   \n",
            "1                  SVM           0.918153            0.000000     0.000000   \n",
            "2                  LDA           0.915986            0.341463     0.052632   \n",
            "\n",
            "   Precision (sklearn)  \n",
            "0             0.000000  \n",
            "1             0.000000  \n",
            "2             0.341463  \n"
          ]
        }
      ],
      "source": [
        "# 16. ADDITIONAL METRICS (ACCURACY, PRECISION, F1) - TEST SET\n",
        "\n",
        "threshold = 0.5\n",
        "\n",
        "metric_rows_test = []\n",
        "for name, probs in prob_predictions_test.items():\n",
        "    preds_binary = (probs >= threshold).astype(int)\n",
        "    metric_rows_test.append({\n",
        "        \"Model\": name,\n",
        "        \"Accuracy (custom)\": accuracy(y_test, probs, threshold),\n",
        "        \"Precision (custom)\": precision(y_test, probs, threshold),\n",
        "        \"F1 (custom)\": f1_score(y_test, probs, threshold),\n",
        "        \"Precision (sklearn)\": precision_score(y_test, preds_binary, zero_division=0),\n",
        "    })\n",
        "\n",
        "metrics_df_test = pd.DataFrame(metric_rows_test)\n",
        "print(\"=== METRICS on TEST SET ===\")\n",
        "print(metrics_df_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== FINAL MODEL COMPARISON (TEST SET) ===\n",
            "              Model  ROC-AUC (Test)  Accuracy (custom)  Precision (custom)  F1 (custom)  Precision (sklearn)\n",
            "Logistic Regression        0.666194           0.918153            0.000000     0.000000             0.000000\n",
            "                SVM        0.500182           0.918153            0.000000     0.000000             0.000000\n",
            "                LDA        0.764865           0.915986            0.341463     0.052632             0.341463\n"
          ]
        }
      ],
      "source": [
        "# 17. SUMMARY - TEST SET PERFORMANCE\n",
        "\n",
        "# Combine all metrics for test set\n",
        "summary_df_test = roc_auc_df_test.merge(metrics_df_test, on=\"Model\")\n",
        "print(\"\\n=== FINAL MODEL COMPARISON (TEST SET) ===\")\n",
        "print(summary_df_test.to_string(index=False))\n",
        "\n",
        "# Note: This evaluation is on the test set using the selected split strategy\n",
        "# To compare split strategies, re-run with different SPLIT_STRATEGY values in cell 12\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
